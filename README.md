# ğŸ” DeBERTa v3 Multi-Hop Retriever

Há»‡ thá»‘ng retrieval Ä‘a bÆ°á»›c sá»­ dá»¥ng DeBERTa v3 vá»›i beam search cho bÃ i toÃ¡n Question Answering trÃªn dataset HotpotQA.

## ğŸ“‹ Tá»•ng quan

Model nÃ y thá»±c hiá»‡n multi-hop reasoning Ä‘á»ƒ tÃ¬m kiáº¿m cÃ¡c supporting facts tá»« nhiá»u documents liÃªn quan Ä‘áº¿n má»™t cÃ¢u há»i. Sá»­ dá»¥ng kiáº¿n trÃºc DeBERTa v3 vá»›i 183M parameters vÃ  beam search algorithm.

### ğŸ¯ TÃ­nh nÄƒng chÃ­nh:
- **DeBERTa v3 Base**: 183,834,628 parameters cho hiá»‡u suáº¥t cao
- **Beam Search**: TÃ¬m kiáº¿m Ä‘a Ä‘Æ°á»ng vá»›i beam_size=2
- **Focal Loss**: Xá»­ lÃ½ class imbalance trong training
- **Multi-hop Reasoning**: Káº¿t há»£p thÃ´ng tin tá»« nhiá»u documents
- **F1 & Exact Match**: ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t chi tiáº¿t

## ğŸ—ï¸ Kiáº¿n trÃºc Model

```
DeBERTa v3 Encoder
â”œâ”€â”€ Question + Context Encoding
â”œâ”€â”€ Hop 1: First document selection
â”œâ”€â”€ Hop 2+: Subsequent document selection  
â”œâ”€â”€ Beam Search Expansion
â””â”€â”€ Supporting Facts Prediction
```

### ğŸ“Š Thá»‘ng kÃª Model:
- **Model**: microsoft/deberta-v3-base
- **Parameters**: 183,834,628
- **Max Sequence Length**: 128-256 tokens
- **Beam Size**: 2
- **Training Data**: HotpotQA

## ğŸš€ CÃ i Ä‘áº·t

### YÃªu cáº§u há»‡ thá»‘ng:
- Python 3.8+
- PyTorch 1.9+
- Transformers 4.20+
- 8GB+ RAM

### CÃ i Ä‘áº·t dependencies:
```bash
# Clone repository
git clone https://github.com/kin1304/BeamRetrieval.git
cd BeamRetrieval

# Táº¡o virtual environment
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# .venv\Scripts\activate   # Windows

# CÃ i Ä‘áº·t packages
pip install -r requirements.txt
```

### Dependencies chÃ­nh:
```
torch>=1.9.0
transformers>=4.20.0
datasets>=2.0.0
tqdm>=4.64.0
tiktoken>=0.4.0
sentencepiece>=0.1.97
```

## ğŸ“ˆ Training

### Quick Start:
```bash
# Training cÆ¡ báº£n vá»›i tham sá»‘ máº·c Ä‘á»‹nh
python train.py

# Training vá»›i cáº¥u hÃ¬nh tÃ¹y chá»‰nh
python train.py --samples 100 --epochs 3 --batch_size 1 --max_len 128
```

### Tham sá»‘ Training:

| Tham sá»‘ | Máº·c Ä‘á»‹nh | MÃ´ táº£ |
|---------|----------|-------|
| `--samples` | 100 | Sá»‘ lÆ°á»£ng training samples |
| `--epochs` | 3 | Sá»‘ epochs training |
| `--batch_size` | 2 | Batch size (khuyáº¿n nghá»‹ 1-2 cho CPU) |
| `--learning_rate` | 1e-5 | Learning rate |
| `--max_len` | 128 | Äá»™ dÃ i sequence tá»‘i Ä‘a |
| `--max_batches` | 10 | Sá»‘ batches tá»‘i Ä‘a má»—i epoch |
| `--save_path` | models/retriever_trained.pt | ÄÆ°á»ng dáº«n lÆ°u model |

### VÃ­ dá»¥ Training Commands:

```bash
# Training nhanh cho testing
python train.py --samples 50 --epochs 2 --batch_size 1 --max_batches 5

# Training á»•n Ä‘á»‹nh
python train.py --samples 100 --epochs 3 --batch_size 1 --max_len 128

# Training Ä‘áº§y Ä‘á»§ (cáº§n nhiá»u thá»i gian)
python train.py --samples 500 --epochs 5 --batch_size 2 --max_len 256
```

## ğŸ“Š Káº¿t quáº£ Training

### Metrics theo dÃµi:
- **F1 Score**: Äo lÆ°á»ng Ä‘á»™ chÃ­nh xÃ¡c partial overlap
- **Exact Match (EM)**: Äo lÆ°á»ng exact match hoÃ n toÃ n
- **Loss**: Cross-entropy hoáº·c Focal loss

### Káº¿t quáº£ máº«u:
```
ğŸ“Š Epoch 3 - Max F1: 0.6667, Max EM: 0.0000, Avg Loss: 0.6787
ğŸ† Best F1: 0.6667, Best EM: 0.0000
ğŸ’¾ Model saved to models/deberta_v3_stable.pt
```

### Giáº£i thÃ­ch Metrics:
- **F1 = 0.6667**: Model dá»± Ä‘oÃ¡n Ä‘Ãºng 2/3 supporting facts
- **EM = 0.0000**: ChÆ°a cÃ³ exact match hoÃ n toÃ n (bÃ¬nh thÆ°á»ng á»Ÿ early training)
- **Loss giáº£m dáº§n**: Model Ä‘ang há»c hiá»‡u quáº£

## ğŸ”§ Cáº¥u trÃºc Code

```
BeamRetrieval/
â”œâ”€â”€ train.py                    # Main training script
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ advanced_retriever.py   # Core model architecture
â”‚   â””â”€â”€ *.pt                    # Trained model checkpoints
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ data_loader.py          # Data loading utilities
â”œâ”€â”€ data/                       # HotpotQA dataset
â””â”€â”€ requirements.txt            # Dependencies
```

### Core Components:

#### 1. **Retriever Class** (`models/advanced_retriever.py`):
```python
class Retriever(nn.Module):
    def __init__(self, model_name="microsoft/deberta-v3-base", beam_size=2, ...)
    def forward(self, q_codes, c_codes, sf_idx, hop=0)
    def encode_texts(self, texts, max_length=None)
```

#### 2. **Training Script** (`train.py`):
```python
def calculate_f1_em(predictions, targets)  # Metrics calculation
def train_epoch(model, dataloader, optimizer, device)  # Training loop
class RetrievalDataset(Dataset)  # Data preparation
```

#### 3. **Data Loader** (`utils/data_loader.py`):
```python
def load_hotpot_data(split='train', sample_size=None)  # HotpotQA loading
```

## ğŸ¯ Sá»­ dá»¥ng Model

### Load trained model:
```python
from models.advanced_retriever import create_advanced_retriever
import torch

# Load model
model = create_advanced_retriever(
    model_name="microsoft/deberta-v3-base",
    beam_size=2,
    use_focal=True,
    max_seq_len=128
)

# Load trained weights
checkpoint = torch.load('models/deberta_v3_stable.pt')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()
```

### Inference example:
```python
# Prepare inputs
question = "What is the capital of France?"
contexts = [
    {"title": "France", "text": "France is a country in Europe..."},
    {"title": "Paris", "text": "Paris is the capital city of France..."},
    # ... more contexts
]

# Tokenize and predict
# (Implementation depends on your specific use case)
```

## ğŸ› Troubleshooting

### Váº¥n Ä‘á» thÆ°á»ng gáº·p:

#### 1. **Memory Issues**:
```bash
# Giáº£m batch_size
python train.py --batch_size 1

# Giáº£m sequence length
python train.py --max_len 64

# Giáº£m sá»‘ samples
python train.py --samples 50
```

#### 2. **Training quÃ¡ cháº­m**:
```bash
# Giá»›i háº¡n batches per epoch
python train.py --max_batches 5

# Sá»­ dá»¥ng Ã­t samples
python train.py --samples 20 --epochs 2
```

#### 3. **Model chá»‰ predict 1 supporting fact**:
- ÄÃ¢y lÃ  hiá»‡n tÆ°á»£ng bÃ¬nh thÆ°á»ng á»Ÿ early training
- Model há»c dáº§n tá»« single-hop sang multi-hop
- TÄƒng epochs vÃ  samples Ä‘á»ƒ cáº£i thiá»‡n

#### 4. **Resource Tracker Warning**:
```
UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects
```
- Warning nÃ y khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n training
- Do memory management cá»§a PyTorch multiprocessing

## ğŸ“ˆ Performance Tips

### ğŸš€ Tá»‘i Æ°u Training:
1. **Batch Size**: Sá»­ dá»¥ng 1-2 cho CPU, 4-8 cho GPU
2. **Sequence Length**: 128 tokens optimal cho speed/accuracy
3. **Gradient Accumulation**: TÄƒng effective batch size
4. **Mixed Precision**: Sá»­ dá»¥ng fp16 náº¿u cÃ³ GPU

### ğŸ’¾ Memory Optimization:
1. **Workspace cleanup**: ÄÃ£ xÃ³a files khÃ´ng cáº§n thiáº¿t
2. **Model checkpointing**: Chá»‰ lÆ°u best models
3. **Data batching**: Xá»­ lÃ½ small batches

## ğŸ“š Dataset

### HotpotQA:
- **Training**: 90,447 samples
- **Dev**: 7,405 samples  
- **Format**: Multi-hop questions vá»›i supporting facts
- **Location**: `data/hotpotqa/`

### Data Structure:
```python
{
    "question": "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?",
    "contexts": [
        {"title": "Document 1", "text": "Content..."},
        {"title": "Document 2", "text": "Content..."}
    ],
    "supporting_facts": [
        ["Document 1", 0],  # Document title, sentence index
        ["Document 2", 3]
    ]
}
```

## ğŸ¤ Contributing

1. Fork repository
2. Táº¡o feature branch
3. Commit changes
4. Push vÃ  táº¡o Pull Request

## ğŸ“„ License

MIT License - xem file LICENSE Ä‘á»ƒ biáº¿t chi tiáº¿t.

## ğŸ™ Acknowledgments

- **DeBERTa v3**: Microsoft Research
- **HotpotQA**: Yang et al., 2018
- **Transformers**: Hugging Face
- **PyTorch**: Facebook Research

---

## ğŸ“ Support

Náº¿u gáº·p váº¥n Ä‘á», vui lÃ²ng:
1. Kiá»ƒm tra [Troubleshooting](#-troubleshooting)
2. Táº¡o issue trÃªn GitHub
3. Äáº£m báº£o Ä‘Ã£ cÃ i Ä‘áº·t Ä‘Ãºng dependencies

**Happy Multi-Hop Reasoning! ğŸ”âœ¨**