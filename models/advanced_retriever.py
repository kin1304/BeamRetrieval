#!/usr/bin/env python3
"""
Model Advanced Multi-Hop Retriever v·ªõi Focal Loss
D·ª±a tr√™n ki·∫øn tr√∫c b·∫°n cung c·∫•p cho h·ªá th·ªëng BeamRetrieval
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import random
import math
from typing import List, Dict, Any, Optional, Tuple
import logging
from transformers import DebertaV2Tokenizer, AutoModel, AutoConfig

logger = logging.getLogger(__name__)

class FocalLoss(nn.Module):
    """
    Tri·ªÉn khai Focal Loss ƒë·ªÉ x·ª≠ l√Ω class imbalance
    """
    def __init__(self, alpha=0.25, gamma=2, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        # T√≠nh cross entropy loss
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')

        # T√≠nh weight (alpha)
        p_t = torch.exp(-ce_loss)
        alpha_t = self.alpha * (1 - p_t)

        # T√≠nh Focal Loss
        focal_loss = alpha_t * (1 - p_t) ** self.gamma * ce_loss

        # Ch·ªçn reduction method
        if self.reduction == 'mean':
            return torch.mean(focal_loss)
        elif self.reduction == 'sum':
            return torch.sum(focal_loss)
        elif self.reduction == 'none':
            return focal_loss
        else:
            raise ValueError("Unsupported reduction mode. Use 'mean', 'sum', or 'none'.")

class Retriever(nn.Module):
    """
    Multi-hop Retriever v·ªõi beam search v√† focal loss
    D·ª±a tr√™n ki·∫øn tr√∫c b·∫°n ch·ªâ ƒë·ªãnh
    """
    
    def __init__(self,
                 config=None,
                 model_name="microsoft/deberta-v3-small",
                 encoder_class=None,
                 max_seq_len=512,
                 mean_passage_len=70,
                 beam_size=2,  # Kh·ªõp v·ªõi y√™u c·∫ßu beam_width=2 c·ªßa b·∫°n
                 gradient_checkpointing=False,
                 use_focal=True,  # B·∫≠t focal loss m·∫∑c ƒë·ªãnh
                 use_early_stop=True,
                 ):
        super().__init__()
        
        # T·∫£i config n·∫øu kh√¥ng ƒë∆∞·ª£c cung c·∫•p
        if config is None:
            config = AutoConfig.from_pretrained(model_name)
        self.config = config
        
        # T·∫£i encoder
        if encoder_class is None:
            from transformers import AutoModel
            encoder_class = AutoModel
        
        self.encoder = encoder_class.from_pretrained(model_name, config=config)
        
        # L∆∞u tr·ªØ parameters
        self.max_seq_len = max_seq_len
        self.mean_passage_len = mean_passage_len
        self.beam_size = beam_size
        # Gradient checkpointing c∆° b·∫£n kh√¥ng t∆∞∆°ng th√≠ch v·ªõi multi-hop reasoning
        # do t√°i s·ª≠ d·ª•ng computational graph gi·ªØa c√°c hop g√¢y l·ªói "backward second time"
        # Thay th·∫ø t·ªëi ∆∞u b·ªô nh·ªõ: s·ª≠ d·ª•ng batch size nh·ªè h∆°n, model parallelism, ho·∫∑c mixed precision
        self.gradient_checkpointing = False
        self.use_focal = use_focal
        self.use_early_stop = use_early_stop
        self.use_label_order = False  # Th√™m thu·ªôc t√≠nh n√†y
        
        # C√°c l·ªõp ph√¢n lo·∫°i cho hop kh√°c nhau
        self.hop_classifier_layer = nn.Linear(config.hidden_size, 2)
        self.hop_n_classifier_layer = nn.Linear(config.hidden_size, 2)
        
        # L∆∞u √Ω: Gradient checkpointing b·ªã v√¥ hi·ªáu h√≥a ƒë·ªÉ t∆∞∆°ng th√≠ch multi-hop
        # C√°c thay th·∫ø t·ªëi ∆∞u b·ªô nh·ªõ: mixed precision, model parallelism, batch sizes nh·ªè h∆°n
        
        # Kh·ªüi t·∫°o tokenizer (ngƒÉn ch·∫∑n c·∫£nh b√°o fast tokenizer cho DeBERTa)
        import warnings
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", message=".*sentencepiece tokenizer.*")
            self.tokenizer = DebertaV2Tokenizer.from_pretrained(model_name)
        
        logger.info(f"üß† Retriever kh·ªüi t·∫°o v·ªõi {self.count_parameters():,} parameters")
        logger.info(f"üìä Beam size: {beam_size}, Max seq len: {max_seq_len}")
        logger.info(f"üéØ S·ª≠ d·ª•ng focal loss: {use_focal}, Early stop: {use_early_stop}")
    
    def count_parameters(self):
        """ƒê·∫øm c√°c parameters c√≥ th·ªÉ train"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
    
    def prepare_question_tokens(self, question_text: str):
        """
        Ph∆∞∆°ng th·ª©c helper ƒë·ªÉ chu·∫©n b·ªã token c√¢u h·ªèi s·∫°ch t·ª´ raw text
        
        Args:
            question_text: Raw text c√¢u h·ªèi
            
        Returns:
            torch.Tensor: Token c√¢u h·ªèi s·∫°ch (kh√¥ng c√≥ [CLS], [SEP], padding)
        """
        # Tokenize c√¢u h·ªèi kh√¥ng c√≥ special tokens
        question_tokens = self.tokenizer(
            question_text,
            add_special_tokens=False,
            return_tensors='pt'
        )['input_ids'].squeeze(0)
        
        return question_tokens
    
    # üóëÔ∏è DEPRECATED: C√°c ph∆∞∆°ng th·ª©c b√™n d∆∞·ªõi kh√¥ng c√≤n ƒë∆∞·ª£c s·ª≠ d·ª•ng sau khi t·ªëi ∆∞u h√≥a
    # Chia paragraph b√¢y gi·ªù ƒë∆∞·ª£c th·ª±c hi·ªán trong data loader (train.py) ƒë·ªÉ hi·ªáu qu·∫£
    
    def _split_context_to_paragraphs(self, context_text: str, max_paragraph_len: int = 200):
        """
        ‚ö†Ô∏è DEPRECATED: Chia context text th√†nh paragraphs ƒë·ªÉ x·ª≠ l√Ω t·ªët h∆°n
        
        üöÄ T·ªêI ∆ØU H√ìA: ƒêi·ªÅu n√†y b√¢y gi·ªù ƒë∆∞·ª£c th·ª±c hi·ªán trong RetrievalDataset._split_context_to_paragraphs()
        ƒë·ªÉ tr√°nh lu·ªìng tokenize ‚Üí decode ‚Üí split ‚Üí tokenize k√©m hi·ªáu qu·∫£.
        
        Args:
            context_text: Full context text
            max_paragraph_len: ƒê·ªô d√†i t·ªëi ƒëa m·ªói paragraph
            
        Returns:
            List c√°c paragraph strings
        """
        # Chia theo c√°c d·∫•u ph√¢n c√°ch paragraph ph·ªï bi·∫øn
        paragraphs = []
        
        # Try splitting by double newlines first
        parts = context_text.split('\n\n')
        
        for part in parts:
            part = part.strip()
            if not part:
                continue
                
            # If part is too long, split by sentences
            if len(part) > max_paragraph_len:
                sentences = part.split('. ')
                current_paragraph = ""
                
                for sentence in sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue
                        
                    # Add period back if not present
                    if not sentence.endswith('.') and not sentence.endswith('!') and not sentence.endswith('?'):
                        sentence += '.'
                    
                    # Check if adding this sentence exceeds limit
                    if len(current_paragraph + sentence) > max_paragraph_len and current_paragraph:
                        paragraphs.append(current_paragraph.strip())
                        current_paragraph = sentence + " "
                    else:
                        current_paragraph += sentence + " "
                
                # Add remaining content
                if current_paragraph.strip():
                    paragraphs.append(current_paragraph.strip())
            else:
                paragraphs.append(part)
        
        # If no paragraphs found, treat entire text as one paragraph
        if not paragraphs:
            paragraphs = [context_text.strip()]
            
        return paragraphs

    def _tokenize_paragraph(self, question_tokens: torch.Tensor, paragraph_text: str):
        """
        ‚ö†Ô∏è DEPRECATED: Tokenize a single paragraph with question to create [CLS] + Q + P + [SEP]
        
        üöÄ OPTIMIZATION: This is now done directly in RetrievalDataset.__getitem__()
        to avoid decode/re-tokenize inefficiency.
        
        Args:
            question_tokens: Token c√¢u h·ªèi (kh√¥ng c√≥ [CLS] v√† [SEP])
            paragraph_text: Text ƒëo·∫°n vƒÉn ƒë·ªÉ tokenize
            
        Returns:
            torch.Tensor: Chu·ªói ƒë√£ tokenized [CLS] + Q + P + [SEP]
        """
        device = question_tokens.device
        
        # Tokenize paragraph text th√¥i (kh√¥ng c√≥ special tokens)
        paragraph_tokens = self.tokenizer(
            paragraph_text,
            add_special_tokens=False,
            return_tensors='pt'
        )['input_ids'].squeeze(0)
        
        # Di chuy·ªÉn l√™n device
        paragraph_tokens = paragraph_tokens.to(device)
        
        # T·∫°o chu·ªói ƒë·∫ßy ƒë·ªß: [CLS] + Q + P + [SEP]
        sequence = torch.cat([
            torch.tensor([self.tokenizer.cls_token_id], device=device),
            question_tokens,
            paragraph_tokens,
            torch.tensor([self.tokenizer.sep_token_id], device=device)
        ])
        
        # C·∫Øt ng·∫Øn n·∫øu qu√° d√†i
        if len(sequence) > self.max_seq_len:
            # Gi·ªØ [CLS] + Q + [SEP], c·∫Øt ng·∫Øn paragraph
            question_with_special = len(question_tokens) + 2  # +2 cho [CLS] v√† [SEP]
            if question_with_special < self.max_seq_len:
                max_paragraph_len = self.max_seq_len - question_with_special
                truncated_paragraph = paragraph_tokens[:max_paragraph_len]
                sequence = torch.cat([
                    torch.tensor([self.tokenizer.cls_token_id], device=device),
                    question_tokens,
                    truncated_paragraph,
                    torch.tensor([self.tokenizer.sep_token_id], device=device)
                ])
            else:
                # C√¢u h·ªèi qu√° d√†i, c·∫Øt ng·∫Øn t·∫•t c·∫£
                sequence = sequence[:self.max_seq_len]
                
        return sequence

    def _extract_paragraph_tokens(self, sequence: torch.Tensor, question_tokens: torch.Tensor):
        """
        Tr√≠ch xu·∫•t token ƒëo·∫°n vƒÉn t·ª´ chu·ªói [CLS] + Q + P + [SEP]
        
        Args:
            sequence: Chu·ªói ƒë√£ tokenized ƒë·∫ßy ƒë·ªß
            question_tokens: Token c√¢u h·ªèi (kh√¥ng c√≥ [CLS] v√† [SEP])
            
        Returns:
            torch.Tensor: Ch·ªâ c√°c token ƒëo·∫°n vƒÉn (ph·∫ßn P)
        """
        # ƒê·ªãnh d·∫°ng chu·ªói: [CLS] + Q + P + [SEP]
        question_start = 1  # B·ªè qua [CLS]
        question_end = question_start + len(question_tokens)
        paragraph_start = question_end
        
        # T√¨m v·ªã tr√≠ [SEP] (n√™n ·ªü cu·ªëi)
        paragraph_end = len(sequence) - 1  # B·ªè qua [SEP] cu·ªëi
        
        # Tr√≠ch xu·∫•t token ƒëo·∫°n vƒÉn
        if paragraph_start < paragraph_end:
            paragraph_tokens = sequence[paragraph_start:paragraph_end]
        else:
            paragraph_tokens = torch.tensor([], device=sequence.device, dtype=sequence.dtype)
            
        return paragraph_tokens

    def _create_multi_hop_sequence(self, question_tokens: torch.Tensor, selected_paragraphs: List[torch.Tensor], new_paragraph: torch.Tensor):
        """
        T·∫°o chu·ªói multi-hop: [CLS] + Q + P1 + P2 + ... + Pnew + [SEP]
        
        Args:
            question_tokens: Token c√¢u h·ªèi (kh√¥ng c√≥ special tokens)
            selected_paragraphs: Danh s√°ch tensor token ƒëo·∫°n vƒÉn ƒë√£ ch·ªçn t·ª´ hop tr∆∞·ªõc
            new_paragraph: Token ƒëo·∫°n vƒÉn m·ªõi ƒë·ªÉ th√™m v√†o
            
        Returns:
            torch.Tensor: Chu·ªói k·∫øt h·ª£p
        """
        device = question_tokens.device
        
        # X√¢y d·ª±ng c√°c ph·∫ßn c·ªßa chu·ªói
        sequence_parts = [
            torch.tensor([self.tokenizer.cls_token_id], device=device),
            question_tokens
        ]
        
        # Th√™m c√°c ƒëo·∫°n vƒÉn ƒë√£ ch·ªçn t·ª´ hop tr∆∞·ªõc
        for paragraph_tokens in selected_paragraphs:
            if len(paragraph_tokens) > 0:
                sequence_parts.append(paragraph_tokens)
        
        # Th√™m ƒëo·∫°n vƒÉn m·ªõi
        if len(new_paragraph) > 0:
            sequence_parts.append(new_paragraph)
            
        # Th√™m [SEP] cu·ªëi
        sequence_parts.append(torch.tensor([self.tokenizer.sep_token_id], device=device))
        
        # K·∫øt h·ª£p t·∫•t c·∫£ c√°c ph·∫ßn
        combined_sequence = torch.cat(sequence_parts)
        
        # C·∫Øt ng·∫Øn n·∫øu qu√° d√†i
        if len(combined_sequence) > self.max_seq_len:
            # Gi·ªØ [CLS] + Q + [SEP], c·∫Øt ng·∫Øn paragraphs t·ª∑ l·ªá
            question_with_special = len(question_tokens) + 2  # +2 cho [CLS] v√† [SEP]
            available_space = self.max_seq_len - question_with_special
            
            if available_space > 0:
                # T√≠nh t·ªïng ƒë·ªô d√†i paragraph
                total_paragraph_len = sum(len(p) for p in selected_paragraphs) + len(new_paragraph)
                
                if total_paragraph_len > available_space:
                    # C·∫Øt ng·∫Øn t·ª´ cu·ªëi
                    combined_sequence = combined_sequence[:self.max_seq_len-1]
                    combined_sequence = torch.cat([
                        combined_sequence, 
                        torch.tensor([self.tokenizer.sep_token_id], device=device)
                    ])
        
        return combined_sequence

    def encode_texts(self, texts: List[str], max_length: int = None):
        """
        Encode danh s√°ch texts s·ª≠ d·ª•ng tokenizer v√† encoder
        """
        if max_length is None:
            max_length = self.max_seq_len
        
        # Tokenize t·∫•t c·∫£ texts
        encoded = self.tokenizer(
            texts,
            return_tensors="pt",
            max_length=max_length,
            truncation=True,
            padding=True
        )
        
        # Di chuy·ªÉn l√™n c√πng device v·ªõi model
        device = next(self.parameters()).device
        encoded = {k: v.to(device) for k, v in encoded.items()}
        
        return encoded
    
    def _hop1_forward(self, hop1_qp_ids, hop1_qp_attention_mask):
        """Forward pass for first hop - simplified without gradient checkpointing"""
        hop1_encoder_outputs = self.encoder(
            input_ids=hop1_qp_ids, 
            attention_mask=hop1_qp_attention_mask
        )[0][:, 0, :]  # [doc_num, hidden_size]
        
        hop1_projection = self.hop_classifier_layer(hop1_encoder_outputs)
        return hop1_projection
    
    def _hop_n_forward(self, hop_qp_ids, hop_qp_attention_mask):
        """Forward pass for subsequent hops - simplified without gradient checkpointing"""
        hop_encoder_outputs = self.encoder(
            input_ids=hop_qp_ids, 
            attention_mask=hop_qp_attention_mask
        )[0][:, 0, :]  # [vec_num, hidden_size]
        
        hop_projection = self.hop_n_classifier_layer(hop_encoder_outputs)
        return hop_projection

    def forward(self, q_codes, p_codes, sf_idx, hop=0, context_mapping=None):
        """
        üöÄ T·ªêI ∆ØU: Forward pass v·ªõi ƒëo·∫°n vƒÉn ƒë√£ chia s·∫µn (kh√¥ng decode/re-tokenize!)
        
        Pipeline:
        1. S·ª≠ d·ª•ng chu·ªói ƒëo·∫°n vƒÉn ƒë√£ chia s·∫µn tr·ª±c ti·∫øp t·ª´ data loader
        2. √Åp d·ª•ng multi-hop reasoning v·ªõi concatenation ph√π h·ª£p
        3. Chuy·ªÉn ƒë·ªïi d·ª± ƒëo√°n ƒëo·∫°n vƒÉn ng∆∞·ª£c v·ªÅ d·ª± ƒëo√°n context
        
        Args:
            q_codes: List ch·ª©a token c√¢u h·ªèi s·∫°ch (kh√¥ng c√≥ [CLS], [SEP])
            p_codes: List c√°c chu·ªói ƒëo·∫°n vƒÉn [CLS] + Q + P + [SEP] (ƒë√£ pre-tokenized)
            sf_idx: Ch·ªâ s·ªë supporting fact (c·∫•p context)
            hop: S·ªë hops cho inference mode
            context_mapping: List √°nh x·∫° ch·ªâ s·ªë ƒëo·∫°n vƒÉn ‚Üí ch·ªâ s·ªë context g·ªëc
        """
        device = q_codes[0].device
        total_loss = torch.tensor(0.0, device=device, requires_grad=True)
        
        # Kh·ªüi t·∫°o bi·∫øn
        loss_function = nn.CrossEntropyLoss()
        
        # üöÄ T·ªêI ∆ØU: S·ª≠ d·ª•ng ƒëo·∫°n vƒÉn ƒë√£ chia s·∫µn tr·ª±c ti·∫øp (kh√¥ng tr√≠ch xu·∫•t context!)
        question_tokens = q_codes[0]  # Token c√¢u h·ªèi s·∫°ch
        all_paragraph_sequences = p_codes  # ƒêo·∫°n vƒÉn ƒë√£ tokenized!
        context_to_paragraph_mapping = context_mapping if context_mapping else list(range(len(p_codes)))
        focal_loss_function = None
        
        if self.use_focal:
            focal_loss_function = FocalLoss()
        
        # üöÄ T·ªêI ∆ØU: Kh√¥ng c√≤n tr√≠ch xu·∫•t context! S·ª≠ d·ª•ng ƒëo·∫°n vƒÉn ƒë√£ chia s·∫µn tr·ª±c ti·∫øp
        # B∆Ø·ªöC 1: ƒê√£ c√≥ chu·ªói ƒëo·∫°n vƒÉn t·ª´ data loader (kh√¥ng decode/re-tokenize!)
        
        # B∆Ø·ªöC 2: X√°c ƒë·ªãnh tham s·ªë training
        if self.training:
            sf_idx = sf_idx[0]
            hops = len(sf_idx)
        else:
            hops = hop if hop > 0 else (len(sf_idx[0]) if sf_idx and len(sf_idx) > 0 else 2)
        
        if len(all_paragraph_sequences) <= hops or hops < 1:
            return {
                'current_preds': [list(range(min(hops, len(all_paragraph_sequences))))],
                'loss': total_loss
            }
        
        # B∆Ø·ªöC 3: Multi-hop reasoning
        current_preds = []  # M·ªói beam theo d√µi ch·ªâ s·ªë ƒëo·∫°n vƒÉn
        selected_paragraph_tokens = []  # M·ªói beam theo d√µi token ƒëo·∫°n vƒÉn ƒë√£ ch·ªçn
        
        for hop_idx in range(hops):
            if hop_idx == 0:
                # HOP ƒê·∫¶U TI√äN: X·ª≠ l√Ω t·∫•t c·∫£ ƒëo·∫°n vƒÉn ƒë·ªôc l·∫≠p
                max_len = max(len(seq) for seq in all_paragraph_sequences)
                num_paragraphs = len(all_paragraph_sequences)
                
                hop1_qp_ids = torch.zeros([num_paragraphs, max_len], device=device, dtype=torch.long)
                hop1_qp_attention_mask = torch.zeros([num_paragraphs, max_len], device=device, dtype=torch.long)
                
                if self.training:
                    hop1_label = torch.zeros([num_paragraphs], dtype=torch.long, device=device)
                
                # ƒêi·ªÅn tensor v·ªõi chu·ªói ƒëo·∫°n vƒÉn
                for i, paragraph_seq in enumerate(all_paragraph_sequences):
                    seq_len = len(paragraph_seq)
                    hop1_qp_ids[i, :seq_len] = paragraph_seq
                    hop1_qp_attention_mask[i, :seq_len] = (paragraph_seq != self.tokenizer.pad_token_id).long()
                    
                    if self.training:
                        # Ki·ªÉm tra xem ƒëo·∫°n vƒÉn n√†y c√≥ thu·ªôc supporting context kh√¥ng
                        original_ctx_idx = context_to_paragraph_mapping[i]
                        if original_ctx_idx in sf_idx:
                            hop1_label[i] = 1
                
                # Forward pass cho hop ƒë·∫ßu ti√™n
                hop1_projection = self._hop1_forward(hop1_qp_ids, hop1_qp_attention_mask)
                
                if self.training:
                    total_loss = total_loss + loss_function(hop1_projection, hop1_label)
                
                # Ch·ªçn top beam_size ƒëo·∫°n vƒÉn
                _, hop1_pred_paragraphs = hop1_projection[:, 1].topk(self.beam_size, dim=-1)
                
                # Kh·ªüi t·∫°o theo d√µi beam
                current_preds = [[idx.item()] for idx in hop1_pred_paragraphs]
                selected_paragraph_tokens = []
                
                for pred_idx in hop1_pred_paragraphs:
                    pred_idx = pred_idx.item()
                    # Tr√≠ch xu·∫•t token ƒëo·∫°n vƒÉn t·ª´ chu·ªói ƒë√£ ch·ªçn
                    selected_seq = all_paragraph_sequences[pred_idx]
                    paragraph_tokens = self._extract_paragraph_tokens(selected_seq, question_tokens)
                    selected_paragraph_tokens.append([paragraph_tokens])
            
            else:
                # HOP TI·∫æP THEO: K·∫øt h·ª£p l·ª±a ch·ªçn tr∆∞·ªõc v·ªõi ·ª©ng vi√™n m·ªõi
                next_sequences = []
                next_labels = []
                next_pred_mapping = []
                
                for beam_idx in range(self.beam_size):
                    beam_selected_paragraphs = selected_paragraph_tokens[beam_idx]
                    beam_used_indices = set(current_preds[beam_idx])
                    
                    # Th·ª≠ t·ª´ng ƒëo·∫°n vƒÉn ch∆∞a s·ª≠ d·ª•ng l√†m ·ª©ng vi√™n ti·∫øp theo
                    for para_idx, paragraph_seq in enumerate(all_paragraph_sequences):
                        if para_idx in beam_used_indices:
                            continue
                        
                        # Tr√≠ch xu·∫•t token ƒëo·∫°n vƒÉn m·ªõi
                        new_paragraph_tokens = self._extract_paragraph_tokens(paragraph_seq, question_tokens)
                        
                        # T·∫°o chu·ªói multi-hop
                        multi_hop_seq = self._create_multi_hop_sequence(
                            question_tokens, 
                            beam_selected_paragraphs, 
                            new_paragraph_tokens
                        )
                        
                        next_sequences.append(multi_hop_seq)
                        next_pred_mapping.append(current_preds[beam_idx] + [para_idx])
                        
                        # Nh√£n cho training
                        if self.training:
                            new_pred_set = set(current_preds[beam_idx] + [para_idx])
                            target_contexts = set()
                            for p_idx in new_pred_set:
                                target_contexts.add(context_to_paragraph_mapping[p_idx])
                            
                            # Ki·ªÉm tra xem t·ªï h·ª£p n√†y c√≥ kh·ªõp v·ªõi supporting facts m·ª•c ti√™u kh√¥ng
                            if target_contexts == set(sf_idx[:hop_idx+1]):
                                next_labels.append(1)
                            else:
                                next_labels.append(0)
                
                if not next_sequences:
                    break
                
                # Chu·∫©n b·ªã tensor cho hop n√†y
                max_len = max(len(seq) for seq in next_sequences)
                num_candidates = len(next_sequences)
                
                hop_qp_ids = torch.zeros([num_candidates, max_len], device=device, dtype=torch.long)
                hop_qp_attention_mask = torch.zeros([num_candidates, max_len], device=device, dtype=torch.long)
                
                if self.training:
                    hop_label = torch.tensor(next_labels, dtype=torch.long, device=device)
                
                # ƒêi·ªÅn tensor
                for i, seq in enumerate(next_sequences):
                    seq_len = len(seq)
                    hop_qp_ids[i, :seq_len] = seq
                    hop_qp_attention_mask[i, :seq_len] = (seq != self.tokenizer.pad_token_id).long()
                
                # Forward pass cho hop ti·∫øp theo
                hop_projection = self._hop_n_forward(hop_qp_ids, hop_qp_attention_mask)
                
                if self.training:
                    if self.use_focal:
                        total_loss = total_loss + focal_loss_function(hop_projection, hop_label)
                    else:
                        total_loss = total_loss + loss_function(hop_projection, hop_label)
                
                # Ch·ªçn top beam_size ·ª©ng vi√™n
                _, hop_pred_indices = hop_projection[:, 1].topk(self.beam_size, dim=-1)
                
                # C·∫≠p nh·∫≠t theo d√µi beam
                new_current_preds = []
                new_selected_paragraph_tokens = []
                
                for pred_idx in hop_pred_indices:
                    pred_idx = pred_idx.item()
                    selected_prediction = next_pred_mapping[pred_idx]
                    new_current_preds.append(selected_prediction)
                    
                    # X√¢y d·ª±ng danh s√°ch token ƒëo·∫°n vƒÉn cho beam n√†y
                    beam_paragraph_tokens = []
                    for para_idx in selected_prediction:
                        para_seq = all_paragraph_sequences[para_idx]
                        para_tokens = self._extract_paragraph_tokens(para_seq, question_tokens)
                        beam_paragraph_tokens.append(para_tokens)
                    
                    new_selected_paragraph_tokens.append(beam_paragraph_tokens)
                
                current_preds = new_current_preds
                selected_paragraph_tokens = new_selected_paragraph_tokens
        
        # Chuy·ªÉn ƒë·ªïi d·ª± ƒëo√°n ƒëo·∫°n vƒÉn ng∆∞·ª£c v·ªÅ d·ª± ƒëo√°n context
        final_context_preds = []
        for beam_paragraphs in current_preds:
            context_indices = []
            for para_idx in beam_paragraphs:
                ctx_idx = context_to_paragraph_mapping[para_idx]
                if ctx_idx not in context_indices:
                    context_indices.append(ctx_idx)
            final_context_preds.append(context_indices)
        
        # Tr·∫£ v·ªÅ k·∫øt qu·∫£
        return {
            'current_preds': final_context_preds,
            'final_preds': final_context_preds,
            'paragraph_preds': current_preds,  # C≈©ng tr·∫£ v·ªÅ d·ª± ƒëo√°n c·∫•p ƒëo·∫°n vƒÉn
            'loss': total_loss
        }
    
    def retrieve_contexts(self, question: str, contexts: List[Dict[str, Any]], max_hops: int = 3):
        """
        Interface c·∫•p cao cho retrieval
        """
        self.eval()
        
        # Chu·∫©n b·ªã ƒë·∫ßu v√†o (ƒë∆°n gi·∫£n h√≥a cho demo)
        # Trong th·ª±c t·∫ø, b·∫°n c·∫ßn tokenize v√† chu·∫©n b·ªã input ƒë√∫ng c√°ch
        with torch.no_grad():
            # ƒê√¢y l√† phi√™n b·∫£n ƒë∆°n gi·∫£n - b·∫°n c·∫ßn implement tokenization ph√π h·ª£p
            # d·ª±a tr√™n y√™u c·∫ßu c·ª• th·ªÉ c·ªßa b·∫°n
            pass
        
        # Tr·∫£ v·ªÅ top contexts (placeholder)
        return contexts[:self.beam_size]
    
    def get_model_summary(self):
        """
        Get comprehensive model summary with parameters and configuration
        """
        summary = {
            'model_info': {
                'total_parameters': self.count_parameters(),
                'model_name': 'microsoft/deberta-v3-base',
                'architecture': 'Advanced Multi-Hop Retriever with Focal Loss'
            },
            'configuration': {
                'max_seq_len': self.max_seq_len,
                'mean_passage_len': self.mean_passage_len,
                'beam_size': self.beam_size,
                'use_focal': self.use_focal,
                'use_early_stop': self.use_early_stop,
                'gradient_checkpointing': self.gradient_checkpointing
            },
            'capabilities': {
                'multi_hop_reasoning': True,
                'beam_search': True,
                'focal_loss': self.use_focal,
                'paragraph_level_processing': True,
                'context_combination': True
            }
        }
        return summary
    
    def print_model_summary(self):
        """Print formatted model summary"""
        summary = self.get_model_summary()
        
        print(f"\nüß† Advanced Multi-Hop Retriever Model Summary")
        print(f"=" * 60)
        
        # Model info
        print(f"üìä Model Information:")
        for key, value in summary['model_info'].items():
            if key == 'total_parameters':
                print(f"   ‚Ä¢ {key.replace('_', ' ').title()}: {value:,}")
            else:
                print(f"   ‚Ä¢ {key.replace('_', ' ').title()}: {value}")
        
        # Configuration
        print(f"\n‚öôÔ∏è  Configuration:")
        for key, value in summary['configuration'].items():
            print(f"   ‚Ä¢ {key.replace('_', ' ').title()}: {value}")
        
        # Capabilities
        print(f"\nüöÄ Capabilities:")
        for key, value in summary['capabilities'].items():
            status = "‚úÖ Enabled" if value else "‚ùå Disabled"
            print(f"   ‚Ä¢ {key.replace('_', ' ').title()}: {status}")
        
        print(f"=" * 60)


# Factory function
def create_advanced_retriever(model_name="microsoft/deberta-v3-small", **kwargs):
    """
    H√†m factory ƒë·ªÉ t·∫°o model advanced retriever
    """
    try:
        config = AutoConfig.from_pretrained(model_name)
        
        # L·ªçc b·ªè c√°c arguments kh√¥ng h·ª£p l·ªá cho Retriever
        valid_args = {}
        retriever_params = {
            'encoder_class', 'max_seq_len', 'mean_passage_len', 
            'beam_size', 'gradient_checkpointing', 'use_focal', 'use_early_stop'
        }
        
        for key, value in kwargs.items():
            if key in retriever_params:
                valid_args[key] = value
        
        # ƒê·∫£m b·∫£o c√≥ beam_size m·∫∑c ƒë·ªãnh
        if 'beam_size' not in valid_args:
            valid_args['beam_size'] = 2
        
        model = Retriever(
            config=config,
            model_name=model_name,
            **valid_args
        )
        return model
    except Exception as e:
        print(f"L·ªói khi t·∫°o model: {e}")
        return None

# Example usage
if __name__ == "__main__":
    # Create model
    model = create_advanced_retriever(
        model_name="microsoft/deberta-v3-base",  # Fallback model
        beam_size=2,
        use_focal=True,
        use_early_stop=True
    )
    
    print(f"üß† Model created with {model.count_parameters():,} parameters")
    print(f"üìä Configuration: beam_size={model.beam_size}, focal_loss={model.use_focal}")
