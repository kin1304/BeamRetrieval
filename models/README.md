# ğŸ§  ThÆ° má»¥c Models - BeamRetrieval

## ğŸ“‹ Tá»•ng quan

ThÆ° má»¥c nÃ y chá»©a cÃ¡c model chÃ­nh cá»§a há»‡ thá»‘ng BeamRetrieval, bao gá»“m implementation cá»§a Advanced Multi-Hop Retriever vá»›i kháº£ nÄƒng beam search vÃ  tá»‘i Æ°u hÃ³a bá»™ nhá»›.

## ğŸ“ Cáº¥u trÃºc Files

### `advanced_retriever.py`
**Model chÃ­nh** - Implementation cá»§a Advanced Multi-Hop Retriever

#### ğŸ¯ CÃ¡c thÃ nh pháº§n chÃ­nh:
- **`FocalLoss`**: Triá»ƒn khai Focal Loss Ä‘á»ƒ xá»­ lÃ½ class imbalance
- **`Retriever`**: Model chÃ­nh vá»›i multi-hop reasoning vÃ  beam search
- **`create_advanced_retriever()`**: Factory function Ä‘á»ƒ táº¡o model

#### ğŸš€ TÃ­nh nÄƒng ná»•i báº­t:
- **Multi-hop reasoning**: Há»— trá»£ reasoning qua nhiá»u hop
- **Beam search**: Theo dÃµi nhiá»u hypothesis vá»›i beam_size=2
- **Paragraph-level processing**: Xá»­ lÃ½ Ä‘oáº¡n vÄƒn riÃªng biá»‡t thay vÃ¬ full context
- **Progressive concatenation**: Káº¿t há»£p thÃ´ng tin tá»« cÃ¡c hop trÆ°á»›c Ä‘Ã³
- **Dual-level output**: Tráº£ vá» cáº£ paragraph-level vÃ  context-level predictions

#### âš™ï¸ Cáº¥u hÃ¬nh Model:
```python
model = create_advanced_retriever(
    model_name="microsoft/deberta-v3-base",
    beam_size=2,
    use_focal=True,
    use_early_stop=True,
    max_seq_len=512,
    gradient_checkpointing=False  # Incompatible vá»›i multi-hop
)
```

#### ğŸ’¾ Model Files (*.pt)
CÃ¡c file model Ä‘Ã£ trained:
- `deberta_v3_full_dataset.pt` - Model train trÃªn full dataset
- `deberta_v3_full_trained.pt` - Model hoÃ n chá»‰nh Ä‘Ã£ train
- `deberta_v3_optimized.pt` - Model tá»‘i Æ°u hÃ³a
- `deberta_v3_paper_dev.pt` - Model train trÃªn dev set (paper config)
- `deberta_v3_paper_full.pt` - Model train vá»›i config paper Ä‘áº§y Ä‘á»§
- `deberta_v3_stable.pt` - PhiÃªn báº£n á»•n Ä‘á»‹nh
- `deberta_v3_trained.pt` - Model Ä‘Ã£ train cÆ¡ báº£n
- `retriever_trained.pt` - Model retriever Ä‘Ã£ train

## ğŸ”§ API chÃ­nh

### Khá»Ÿi táº¡o Model
```python
from models.advanced_retriever import create_advanced_retriever

model = create_advanced_retriever(
    model_name="microsoft/deberta-v3-base",
    beam_size=2,
    use_focal=True,
    max_seq_len=512
)
```

### Forward Pass
```python
outputs = model(
    q_codes=question_tokens,      # Token cÃ¢u há»i sáº¡ch
    p_codes=paragraph_sequences,  # Chuá»—i Ä‘oáº¡n vÄƒn Ä‘Ã£ tokenized
    sf_idx=supporting_facts,      # Supporting fact indices
    hop=2,                       # Sá»‘ hops
    context_mapping=mapping      # Ãnh xáº¡ paragraphâ†’context
)
```

### Output Format
```python
{
    'current_preds': [[context_indices]],     # Context-level predictions
    'final_preds': [[context_indices]],       # Alias cho backward compatibility
    'paragraph_preds': [[paragraph_indices]], # Paragraph-level predictions (chi tiáº¿t)
    'loss': tensor(loss_value)                # Training loss
}
```

## ğŸ—ï¸ Kiáº¿n trÃºc Algorithm

### Hop 1: Independent Paragraph Scoring
- Cháº¥m Ä‘iá»ƒm tá»«ng Ä‘oáº¡n vÄƒn Ä‘á»™c láº­p vá»›i cÃ¢u há»i
- Format: `[CLS] + Question + Paragraph + [SEP]`
- Chá»n top `beam_size` Ä‘oáº¡n vÄƒn cÃ³ Ä‘iá»ƒm cao nháº¥t

### Hop 2+: Multi-hop Combination
- Má»Ÿ rá»™ng má»—i beam vá»›i táº¥t cáº£ Ä‘oáº¡n vÄƒn chÆ°a sá»­ dá»¥ng
- Progressive concatenation: `[CLS] + Q + P1 + P2 + ... + [SEP]`
- Beam pruning: Chá»‰ giá»¯ top candidates

### Output Generation
- Chuyá»ƒn Ä‘á»•i paragraph predictions vá» context predictions
- Dual-level output cho flexibility

## ğŸ”¥ Tá»‘i Æ°u hÃ³a

### Memory Optimization
- **Gradient checkpointing**: Disabled (incompatible vá»›i multi-hop)
- **Mixed precision**: Há»— trá»£ automatic mixed precision
- **Paragraph splitting**: Pre-processing trong data loader
- **Device management**: Consistent GPU usage

### Performance Features
- **Vectorized operations**: Batch processing hiá»‡u quáº£
- **Smart truncation**: Æ¯u tiÃªn question vÃ  special tokens
- **Progressive reasoning**: TÃ­ch lÅ©y context qua cÃ¡c hop

## ğŸ“Š Metrics vÃ  Evaluation

Model Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ báº±ng:
- **F1 Score**: Äo overlap má»™t pháº§n giá»¯a predicted vÃ  target
- **Exact Match (EM)**: Perfect match vá»›i supporting facts
- **Loss Tracking**: CrossEntropy + Focal Loss (optional)

## ğŸš€ Sá»­ dá»¥ng

### Training
```bash
python train.py --dataset train --epochs 16 --batch_size 1 --learning_rate 2e-5
```

### Inference
```python
model.eval()
with torch.no_grad():
    outputs = model(q_codes, p_codes, sf_idx, hop=2)
    predictions = outputs['current_preds'][0]
```

## ğŸ“ LÆ°u Ã½ quan trá»ng

1. **Gradient Checkpointing**: KhÃ´ng tÆ°Æ¡ng thÃ­ch vá»›i multi-hop reasoning do reuse computational graph
2. **Memory Management**: Sá»­ dá»¥ng mixed precision hoáº·c smaller batch size thay vÃ¬ gradient checkpointing  
3. **Device Consistency**: Äáº£m báº£o táº¥t cáº£ tensors cÃ¹ng device
4. **Paragraph Processing**: Paragraph splitting Ä‘Æ°á»£c thá»±c hiá»‡n trong data loader Ä‘á»ƒ hiá»‡u quáº£

## ğŸ”— Dependencies

- PyTorch â‰¥ 1.9.0
- Transformers â‰¥ 4.20.0
- DeBERTa-v3-base model tá»« Hugging Face

## ğŸ“ˆ Performance

Model Ä‘áº¡t Ä‘Æ°á»£c:
- **F1 Score**: ~66.7% trÃªn dev set
- **Exact Match**: ~40-50% tÃ¹y thuá»™c vÃ o dataset
- **Training Speed**: ~2-3 samples/second trÃªn Apple Silicon MPS
