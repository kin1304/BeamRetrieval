# üõ†Ô∏è Th∆∞ m·ª•c Utils - BeamRetrieval

## üìã T·ªïng quan

Th∆∞ m·ª•c n√†y ch·ª©a c√°c utility functions v√† helpers h·ªó tr·ª£ cho h·ªá th·ªëng BeamRetrieval, bao g·ªìm data loading, processing v√† c√°c c√¥ng c·ª• ti·ªán √≠ch kh√°c.

## üìÅ C·∫•u tr√∫c Files

### `data_loader.py`
**Module ch√≠nh** - X·ª≠ l√Ω load v√† preprocessing d·ªØ li·ªáu

#### üéØ C√°c class ch√≠nh:

##### `DatasetLoader`
Class ch√≠nh ƒë·ªÉ load v√† x·ª≠ l√Ω c√°c lo·∫°i dataset kh√°c nhau cho Multi-Hop QA

**T√≠nh nƒÉng:**
- H·ªó tr·ª£ nhi·ªÅu format: JSON, CSV, TSV
- X·ª≠ l√Ω dataset chu·∫©n nh∆∞ HotpotQA
- Validation v√† error handling
- Logging chi ti·∫øt

##### `HotpotQAProcessor`  
Processor chuy√™n bi·ªát cho dataset HotpotQA

**T√≠nh nƒÉng:**
- Parse format HotpotQA ch√≠nh x√°c
- Extract supporting facts
- Normalize d·ªØ li·ªáu
- Context v√† question preprocessing

### `__init__.py`
Package initialization file ƒë·ªÉ import modules

## üîß API ch√≠nh

### DatasetLoader

#### Kh·ªüi t·∫°o
```python
from utils.data_loader import DatasetLoader

loader = DatasetLoader()
```

#### Load JSON Dataset
```python
data = loader.load_json("data/hotpotqa/hotpot_train_v1.1.json")
print(f"ƒê√£ load {len(data)} samples")
```

#### Load CSV/TSV Dataset
```python
# CSV file
data = loader.load_csv("dataset.csv", delimiter=',')

# TSV file  
data = loader.load_csv("dataset.tsv", delimiter='\t')
```

#### Load HotpotQA Format
```python
data = loader.load_hotpotqa_format("data/hotpotqa/hotpot_dev_distractor_v1.json")
```

### HotpotQA Processing

#### Load HotpotQA Data
```python
from utils.data_loader import load_hotpot_data

# Load training data
train_data = load_hotpot_data('train', sample_size=1000)

# Load development data  
dev_data = load_hotpot_data('dev', sample_size=500)

# Load full dataset
full_data = load_hotpot_data('train')  # sample_size=None
```

#### Dataset Format
M·ªói sample c√≥ structure:
```python
{
    'question': str,                    # C√¢u h·ªèi
    'answer': str,                      # C√¢u tr·∫£ l·ªùi
    'contexts': [                       # Danh s√°ch contexts
        {
            'title': str,               # Ti√™u ƒë·ªÅ context
            'text': str                 # N·ªôi dung context
        }
    ],
    'supporting_facts': [               # Supporting facts
        [title, sentence_idx],          # [t√™n context, ch·ªâ s·ªë c√¢u]
    ],
    'type': str,                        # Lo·∫°i c√¢u h·ªèi ('bridge', 'comparison')
    'level': str                        # ƒê·ªô kh√≥ ('easy', 'medium', 'hard')
}
```

## üöÄ Data Processing Features

### Preprocessing Capabilities
- **Text normalization**: Chu·∫©n h√≥a text input
- **Context splitting**: Chia context th√†nh paragraphs
- **Token management**: X·ª≠ l√Ω special tokens
- **Supporting facts extraction**: Parse ch√≠nh x√°c supporting facts

### Validation & Error Handling
- **Data integrity checks**: Ki·ªÉm tra t√≠nh to√†n v·∫πn d·ªØ li·ªáu
- **Format validation**: Validate format ƒë√∫ng chu·∫©n
- **Error logging**: Log chi ti·∫øt c√°c l·ªói
- **Fallback mechanisms**: X·ª≠ l√Ω l·ªói graceful

### Performance Optimization
- **Lazy loading**: Load d·ªØ li·ªáu khi c·∫ßn
- **Memory efficient**: T·ªëi ∆∞u s·ª≠ d·ª•ng b·ªô nh·ªõ
- **Caching**: Cache d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
- **Batch processing**: X·ª≠ l√Ω theo batch

## üìä Dataset Statistics

### HotpotQA Dataset
- **Training set**: ~90,000 samples
- **Development set**: ~7,400 samples  
- **Test set**: ~7,400 samples
- **Question types**: Bridge, Comparison
- **Difficulty levels**: Easy, Medium, Hard

### Context Information
- **Average contexts per question**: 10
- **Average context length**: 200-500 tokens
- **Supporting facts**: 2-3 per question
- **Multi-hop reasoning**: 2-4 hops typically

## üîç Usage Examples

### C∆° b·∫£n
```python
from utils.data_loader import load_hotpot_data

# Load d·ªØ li·ªáu training
data = load_hotpot_data('train', sample_size=1000)

# Duy·ªát qua samples
for sample in data:
    question = sample['question']
    contexts = sample['contexts'] 
    supporting_facts = sample['supporting_facts']
    
    print(f"Q: {question}")
    print(f"Supporting facts: {len(supporting_facts)}")
    print(f"Contexts: {len(contexts)}")
```

### Advanced Processing
```python
from utils.data_loader import DatasetLoader, HotpotQAProcessor

# Kh·ªüi t·∫°o
loader = DatasetLoader()
processor = HotpotQAProcessor()

# Load v√† preprocess
raw_data = loader.load_json("hotpot_train_v1.1.json")
processed_data = processor.process_batch(raw_data)

# Validate d·ªØ li·ªáu
validated_data = processor.validate_samples(processed_data)
print(f"Valid samples: {len(validated_data)}/{len(processed_data)}")
```

### Custom Dataset
```python
# T·∫°o custom dataset format
custom_data = [
    {
        'question': 'Your question here',
        'contexts': [
            {'title': 'Context 1', 'text': 'Content...'},
            {'title': 'Context 2', 'text': 'Content...'}
        ],
        'supporting_facts': [['Context 1', 0], ['Context 2', 1]],
        'answer': 'Answer here'
    }
]

# Process nh∆∞ HotpotQA
processor = HotpotQAProcessor()
processed = processor.process_batch(custom_data)
```

## üîß Configuration

### Logging Configuration
```python
import logging

# Set log level
logging.getLogger('utils.data_loader').setLevel(logging.DEBUG)

# Custom log format
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
```

### Memory Management
```python
# C·∫•u h√¨nh memory limits
loader = DatasetLoader()
loader.set_memory_limit(8)  # GB
loader.enable_lazy_loading(True)
```

## üìù Best Practices

### Performance Tips
1. **S·ª≠ d·ª•ng sample_size**: Limit d·ªØ li·ªáu khi development
2. **Enable caching**: Cache processed data
3. **Batch processing**: Process nhi·ªÅu samples c√πng l√∫c
4. **Memory monitoring**: Theo d√µi memory usage

### Error Handling
1. **Validate input**: Ki·ªÉm tra format tr∆∞·ªõc khi process
2. **Log errors**: Log chi ti·∫øt ƒë·ªÉ debug
3. **Fallback data**: C√≥ backup data khi l·ªói
4. **Graceful degradation**: Ti·∫øp t·ª•c khi c√≥ l·ªói nh·ªè

### Data Quality
1. **Check supporting facts**: ƒê·∫£m b·∫£o supporting facts valid
2. **Validate contexts**: Ki·ªÉm tra contexts c√≥ ƒë·ªß th√¥ng tin
3. **Question quality**: Filter c√¢u h·ªèi quality th·∫•p
4. **Answer consistency**: ƒê·∫£m b·∫£o answer consistent

## üîó Dependencies

- Python ‚â• 3.7
- JSON (built-in)
- CSV (built-in)  
- Logging (built-in)
- Typing (built-in)

## üìà Performance Metrics

### Loading Speed
- **JSON**: ~1000 samples/second
- **CSV**: ~800 samples/second
- **HotpotQA**: ~500 samples/second (v·ªõi validation)

### Memory Usage
- **Raw data**: ~100MB cho 10K samples
- **Processed data**: ~150MB cho 10K samples
- **Peak memory**: ~300MB during processing

## üõ†Ô∏è Troubleshooting

### Common Issues

#### Memory Errors
```python
# Solution: S·ª≠ d·ª•ng smaller sample_size
data = load_hotpot_data('train', sample_size=1000)
```

#### Format Errors  
```python
# Solution: Validate tr∆∞·ªõc khi load
loader = DatasetLoader()
if loader.validate_format(file_path):
    data = loader.load_json(file_path)
```

#### Encoding Issues
```python
# Solution: Specify encoding
with open(file_path, 'r', encoding='utf-8') as f:
    data = json.load(f)
```

## üìû Support

N·∫øu g·∫∑p v·∫•n ƒë·ªÅ v·ªõi data loading:
1. Ki·ªÉm tra log messages
2. Validate input data format
3. Verify file permissions
4. Check memory availability
5. Review dataset documentation
