#!/usr/bin/env python3
"""
Script Training ChÃ­nh cho Advanced Multi-Hop Retriever
CÃ¡ch dÃ¹ng: python tra                # ThÃªm ná»™i dung cÃ²n láº¡i
                if current_paragraph.strip():
                    paragraphs.append(current_paragraph.strip())
        
        # Lá»c bá» Ä‘oáº¡n vÄƒn rá»—ng vÃ  Ä‘áº£m báº£o ná»™i dung tá»‘i thiá»ƒu
        paragraphs = [p for p in paragraphs if len(p.strip()) > 10]
        
        # Äáº£m báº£o Ã­t nháº¥t má»™t Ä‘oáº¡n vÄƒn
        if not paragraphs:
            paragraphs = [context_text[:max_len] if context_text else "KhÃ´ng cÃ³ ná»™i dung."]ataset train/dev] [--samples N] [--epochs N] [--batch_size N] [--gpu]
"""
import argparse
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import DebertaV2Tokenizer
from tqdm import tqdm
import random

from models.advanced_retriever import create_advanced_retriever
from utils.data_loader import load_hotpot_data

def get_device():
    """Láº¥y thiáº¿t bá»‹ tá»‘t nháº¥t cÃ³ sáºµn"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"ğŸš€ Using GPU: {torch.cuda.get_device_name()}")
        print(f"ğŸ”¥ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    elif torch.backends.mps.is_available():
        device = torch.device('mps')
        print("ğŸ Using Apple Silicon GPU (MPS)")
    else:
        device = torch.device('cpu')
        print("ğŸ’» Using CPU")
    return device

def calculate_f1_em(predictions, targets):
    """TÃ­nh toÃ¡n F1 score vÃ  Exact Match"""
    if not predictions or not targets:
        return 0.0, 0.0
    
    # Chuyá»ƒn Ä‘á»•i thÃ nh sets Ä‘á»ƒ tÃ­nh F1
    pred_set = set(predictions)
    target_set = set(targets)
    
    # Exact Match
    em = 1.0 if pred_set == target_set else 0.0
    
    # F1 Score
    if len(pred_set) == 0 and len(target_set) == 0:
        f1 = 1.0
    elif len(pred_set) == 0 or len(target_set) == 0:
        f1 = 0.0
    else:
        precision = len(pred_set & target_set) / len(pred_set)
        recall = len(pred_set & target_set) / len(target_set)
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    
    return f1, em

class RetrievalDataset(Dataset):
    """Dataset cho training multi-hop retrieval"""
    
    def __init__(self, data, tokenizer, max_len=256, num_contexts=5):
        self.data = data
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.num_contexts = num_contexts
        
    def __len__(self):
        return len(self.data)
    
    def _split_context_to_paragraphs(self, context_text, max_len=200):
        """
        ğŸš€ Tá»I Æ¯U: Chia context thÃ nh Ä‘oáº¡n vÄƒn trá»±c tiáº¿p tá»« raw text
        KhÃ´ng cÃ³ hiá»‡n tÆ°á»£ng decode/re-tokenize kÃ©m hiá»‡u quáº£!
        """
        paragraphs = []
        
        # BÆ°á»›c 1: Chia theo double newlines (ngáº¯t Ä‘oáº¡n vÄƒn tá»± nhiÃªn)
        parts = context_text.split('\n\n')
        
        for part in parts:
            part = part.strip()
            if not part:
                continue
                
            if len(part) <= max_len:
                paragraphs.append(part)
            else:
                # BÆ°á»›c 2: Chia cÃ¡c pháº§n dÃ i theo cÃ¢u
                sentences = part.split('. ')
                current_paragraph = ""
                
                for sentence in sentences:
                    # Kiá»ƒm tra viá»‡c thÃªm cÃ¢u nÃ y cÃ³ vÆ°á»£t quÃ¡ max_len khÃ´ng
                    test_paragraph = current_paragraph + sentence + ". " if current_paragraph else sentence + ". "
                    
                    if len(test_paragraph) > max_len and current_paragraph:
                        # LÆ°u Ä‘oáº¡n vÄƒn hiá»‡n táº¡i vÃ  báº¯t Ä‘áº§u Ä‘oáº¡n má»›i
                        paragraphs.append(current_paragraph.strip())
                        current_paragraph = sentence + ". "
                    else:
                        current_paragraph = test_paragraph
                
                # ThÃªm ná»™i dung cÃ²n láº¡i
                if current_paragraph.strip():
                    paragraphs.append(current_paragraph.strip())
        
        # Filter out empty paragraphs and ensure minimum content
        paragraphs = [p for p in paragraphs if len(p.strip()) > 10]
        
        # Ensure at least one paragraph
        if not paragraphs:
            paragraphs = [context_text[:max_len] if context_text else "No content available."]
        
        return paragraphs
    
    def __getitem__(self, idx):
        item = self.data[idx]
        question = item['question']
        contexts = item['contexts']
        supporting_facts = item.get('supporting_facts', [])
        
        # Chá»n contexts
        if len(contexts) > self.num_contexts:
            sf_titles = {sf[0] for sf in supporting_facts}
            sf_contexts = [ctx for ctx in contexts if ctx['title'] in sf_titles]
            other_contexts = [ctx for ctx in contexts if ctx['title'] not in sf_titles]
            
            selected_contexts = sf_contexts[:2]
            remaining = self.num_contexts - len(selected_contexts)
            if remaining > 0 and other_contexts:
                selected_contexts.extend(random.sample(other_contexts, min(remaining, len(other_contexts))))
        else:
            selected_contexts = contexts
        
        # Äá»‡m contexts náº¿u cáº§n
        while len(selected_contexts) < self.num_contexts:
            selected_contexts.append({'title': 'Empty', 'text': 'KhÃ´ng cÃ³ context.'})
        
        # ğŸš€ Tá»I Æ¯U: Chia Ä‘oáº¡n vÄƒn TRÆ¯á»šC khi tokenization (khÃ´ng decode/re-tokenize!)
        q_tokens_list = []
        p_tokens_list = []  # Má»›i: chuá»—i Ä‘oáº¡n vÄƒn trá»±c tiáº¿p
        context_to_paragraph_mapping = []  # Ãnh xáº¡ chá»‰ sá»‘ Ä‘oáº¡n vÄƒn tá»›i chá»‰ sá»‘ context gá»‘c
        
        # Tokenize cÃ¢u há»i má»™t láº§n (token sáº¡ch khÃ´ng cÃ³ special tokens)
        question_tokens = self.tokenizer(
            question,
            max_length=self.max_len // 2,
            truncation=True,
            add_special_tokens=False,  # KhÃ´ng cÃ³ [CLS], [SEP] cho token sáº¡ch
            return_tensors='pt'
        )['input_ids'].view(-1)
        
        # Äá»‡m question tokens tá»›i Ä‘á»™ dÃ i nháº¥t quÃ¡n
        q_max_len = self.max_len // 2
        if len(question_tokens) > q_max_len:
            question_tokens = question_tokens[:q_max_len]
        else:
            padding_len = q_max_len - len(question_tokens)
            question_tokens = torch.cat([question_tokens, torch.full((padding_len,), self.tokenizer.pad_token_id)])
        
        # Xá»­ lÃ½ tá»«ng context vÃ  chia thÃ nh Ä‘oáº¡n vÄƒn
        for ctx_idx, ctx in enumerate(selected_contexts[:self.num_contexts]):
            ctx_text = f"{ctx['title']}: {ctx['text']}"
            
            # Chia context thÃ nh Ä‘oáº¡n vÄƒn TRÆ¯á»šC tokenization (khÃ´ng máº¥t thÃ´ng tin!)
            paragraphs = self._split_context_to_paragraphs(ctx_text)
            
            # Tokenize tá»«ng Ä‘oáº¡n vÄƒn trá»±c tiáº¿p: [CLS] + Q + P + [SEP]
            for paragraph_text in paragraphs:
                combined_text = question + " " + paragraph_text
                para_tokens = self.tokenizer(
                    combined_text,
                    max_length=self.max_len,
                    truncation=True,
                    padding='max_length',
                    return_tensors='pt'
                )['input_ids'].view(-1)
                
                p_tokens_list.append(para_tokens)
                context_to_paragraph_mapping.append(ctx_idx)
        
        # LÆ°u question tokens má»™t láº§n (dÃ¹ng láº¡i cho táº¥t cáº£ Ä‘oáº¡n vÄƒn)
        q_tokens_list.append(question_tokens)
        
        # Chá»‰ sá»‘ supporting facts (Ã¡nh xáº¡ tá»›i chá»‰ sá»‘ context gá»‘c)
        sf_indices = []
        for i, ctx in enumerate(selected_contexts[:self.num_contexts]):
            if any(ctx['title'] == sf[0] for sf in supporting_facts):
                sf_indices.append(i)
        
        # Äáº£m báº£o Ã­t nháº¥t 1 supporting fact
        if not sf_indices:
            sf_indices.append(0)
        sf_indices = sf_indices[:3]  # Tá»‘i Ä‘a 3 supporting facts
        
        # Äá»‡m Ä‘á»ƒ Ä‘áº£m báº£o Ä‘á»™ dÃ i nháº¥t quÃ¡n
        while len(sf_indices) < 2:
            sf_indices.append(sf_indices[0])
        
        return {
            'q_codes': q_tokens_list,  # Token cÃ¢u há»i sáº¡ch Ä‘Æ¡n (khÃ´ng cÃ³ [CLS], [SEP])
            'p_codes': p_tokens_list,  # Má»šI: Chuá»—i Ä‘oáº¡n vÄƒn trá»±c tiáº¿p [CLS] + Q + P + [SEP]
            'context_mapping': context_to_paragraph_mapping,  # Má»šI: Ãnh xáº¡ Äoáº¡n vÄƒn â†’ Context
            'sf_idx': [torch.tensor(sf_indices, dtype=torch.long)],
            'hop': len(sf_indices)
        }

def collate_fn(batch):
    """ğŸš€ Tá»I Æ¯U: HÃ m collate tÃ¹y chá»‰nh cho Ä‘á»‹nh dáº¡ng paragraph-based má»›i"""
    return {
        'q_codes': [item['q_codes'] for item in batch],
        'p_codes': [item['p_codes'] for item in batch],  # Má»šI: Chuá»—i Ä‘oáº¡n vÄƒn
        'context_mapping': [item['context_mapping'] for item in batch],  # Má»šI: Ãnh xáº¡ Äoáº¡n vÄƒn â†’ Context
        'sf_idx': [item['sf_idx'] for item in batch],
        'hops': [item['hop'] for item in batch]
    }

def train_epoch(model, dataloader, optimizer, device, max_batches=None, scaler=None):
    """Train má»™t epoch vá»›i tá»‘i Æ°u hÃ³a GPU"""
    model.train()
    epoch_losses = []
    f1_scores = []
    em_scores = []
    
    # TÃ­nh toÃ¡n checkpoint 30% Ä‘á»ƒ bÃ¡o cÃ¡o
    total_batches = max_batches if max_batches else len(dataloader)
    checkpoint_30_percent = int(total_batches * 0.3)
    
    progress_bar = tqdm(dataloader, desc="Training")
    for batch_idx, batch in enumerate(progress_bar):
        
        batch_f1 = 0
        batch_em = 0
        valid_samples = 0
        
        # Xá»­ lÃ½ tá»«ng sample trong batch riÃªng láº»
        for i in range(len(batch['q_codes'])):
            try:
                # Zero gradients cho tá»«ng sample
                optimizer.zero_grad()
                
                # ğŸš€ Tá»I Æ¯U: Di chuyá»ƒn dá»¯ liá»‡u lÃªn device hiá»‡u quáº£ (Ä‘á»‹nh dáº¡ng má»›i)
                q_codes = [q.to(device, non_blocking=True) for q in batch['q_codes'][i]]
                p_codes = [p.to(device, non_blocking=True) for p in batch['p_codes'][i]]  # Má»šI: Chuá»—i Ä‘oáº¡n vÄƒn
                context_mapping = batch['context_mapping'][i]  # Má»šI: ThÃ´ng tin Ã¡nh xáº¡
                sf_idx = [s.to(device, non_blocking=True) for s in batch['sf_idx'][i]]
                hop = batch['hops'][i]
                
                # Mixed precision forward pass
                if scaler is not None:

                    with torch.cuda.amp.autocast('cuda'):
                        # ğŸš€ Tá»I Æ¯U: Sá»­ dá»¥ng p_codes (chuá»—i Ä‘oáº¡n vÄƒn) vÃ  context_mapping
                        outputs = model(q_codes, p_codes, sf_idx, hop, context_mapping=context_mapping)
                        loss = outputs['loss']
                else:
                    outputs = model(q_codes, p_codes, sf_idx, hop, context_mapping=context_mapping)
                    loss = outputs['loss']
                
                if loss.requires_grad and not torch.isnan(loss):
                    # Backward pass cho sample riÃªng láº»
                    if scaler is not None:
                        scaler.scale(loss).backward()
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        loss.backward()
                        optimizer.step()
                    
                    valid_samples += 1
                    epoch_losses.append(loss.item())
                    
                    # TÃ­nh toÃ¡n F1 vÃ  EM chá»‰ sau khi Táº¤T Cáº¢ hops Ä‘Ã£ hoÃ n thÃ nh
                    predictions = []
                    targets = sf_idx[0].cpu().tolist()
                    
                    if 'final_preds' in outputs and outputs['final_preds']:
                        # Sá»­ dá»¥ng final predictions sau khi táº¥t cáº£ hops hoÃ n thÃ nh
                        predictions = outputs['final_preds'][0] if len(outputs['final_preds']) > 0 else []
                        
                    elif 'current_preds' in outputs and outputs['current_preds']:
                        # Fallback tá»›i current predictions náº¿u final khÃ´ng cÃ³
                        predictions = outputs['current_preds'][0] if len(outputs['current_preds']) > 0 else []
                    
                    # LuÃ´n tÃ­nh F1/EM ká»ƒ cáº£ khi predictions rá»—ng (Ä‘á»ƒ debug)
                    f1, em = calculate_f1_em(predictions, targets)
                    batch_f1 += f1
                    batch_em += em
                    f1_scores.append(f1)
                    em_scores.append(em)
                    
                    # ThÃ´ng tin debug cho vÃ i sample Ä‘áº§u
                    if len(f1_scores) <= 3:
                        print(f"\nğŸ” Debug Sample {len(f1_scores)}:")
                        print(f"   Predictions: {predictions}")
                        print(f"   Targets: {targets}")
                        print(f"   F1: {f1:.4f}, EM: {em:.4f}")
                        print(f"   Available outputs: {list(outputs.keys())}")
                    
            except Exception as e:
                print(f"Sample {i} tháº¥t báº¡i: {e}")
                continue
        
        if valid_samples > 0:
            avg_f1 = batch_f1 / valid_samples
            avg_em = batch_em / valid_samples
            
            progress_bar.set_postfix({
                'loss': f'{epoch_losses[-1]:.4f}' if epoch_losses else '0.0000',
                'f1': f'{avg_f1:.4f}',
                'em': f'{avg_em:.4f}',
                'valid': f'{valid_samples}/{len(batch["q_codes"])}'
            })
        
        # XÃ³a GPU cache Ä‘á»‹nh ká»³
        if torch.cuda.is_available() and batch_idx % 10 == 0:
            torch.cuda.empty_cache()
        
        # BÃ¡o cÃ¡o metrics táº¡i checkpoint 30%
        if batch_idx == checkpoint_30_percent and f1_scores and em_scores:
            current_avg_f1 = sum(f1_scores) / len(f1_scores)
            current_avg_em = sum(em_scores) / len(em_scores)
            current_avg_loss = sum(epoch_losses) / len(epoch_losses)
            print(f"\nğŸ“Š 30% Checkpoint ({batch_idx+1}/{total_batches} batches):")
            print(f"   Average Loss: {current_avg_loss:.4f}")
            print(f"   Average F1: {current_avg_f1:.4f}")
            print(f"   Average EM: {current_avg_em:.4f}")
            print(f"   Max F1 so far: {max(f1_scores):.4f}")
            print(f"   Max EM so far: {max(em_scores):.4f}")
        
        # Dá»«ng sá»›m náº¿u max_batches Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh
        if max_batches and batch_idx >= max_batches - 1:
            break
    
    max_f1 = max(f1_scores) if f1_scores else 0.0
    max_em = max(em_scores) if em_scores else 0.0
    avg_loss = sum(epoch_losses) / len(epoch_losses) if epoch_losses else 0.0
    
    return max_f1, max_em, avg_loss

def main():
    parser = argparse.ArgumentParser(description='Train Advanced Multi-Hop Retriever - Cáº¥u hÃ¬nh Paper')
    # ========== Cáº¤U HÃŒNH PAPER (GIÃ TRá»Š Máº¶C Äá»ŠNH) ==========
    parser.add_argument('--dataset', type=str, default='train', choices=['train', 'dev'], 
                       help='Dataset sá»­ dá»¥ng cho training (train hoáº·c dev)')
    parser.add_argument('--samples', type=int, default=None, help='Sá»‘ lÆ°á»£ng training samples (None = FULL DATASET)')
    parser.add_argument('--epochs', type=int, default=2, help='Sá»‘ epochs (Paper: 16, Test: 2)')
    parser.add_argument('--batch_size', type=int, default=1, help='KÃ­ch thÆ°á»›c batch (Paper: 1)')
    parser.add_argument('--learning_rate', type=float, default=2e-5, help='Learning rate (Paper: 2e-5)')
    parser.add_argument('--max_len', type=int, default=512, help='Äá»™ dÃ i chuá»—i tá»‘i Ä‘a (Paper: 512)')
    parser.add_argument('--save_path', type=str, default='models/deberta_v3_paper_full.pt', help='ÄÆ°á»ng dáº«n lÆ°u model')
    parser.add_argument('--gradient_accumulation', type=int, default=1, help='BÆ°á»›c tÃ­ch lÅ©y gradient')
    parser.add_argument('--max_batches', type=int, default=None, help='Sá»‘ batch tá»‘i Ä‘a má»—i epoch (None = FULL DATASET)')
    parser.add_argument('--gpu', action='store_true', help='Ã‰p buá»™c sá»­ dá»¥ng GPU')
    parser.add_argument('--mixed_precision', action='store_true', default=False, help='Sá»­ dá»¥ng mixed precision (thÆ°á»ng False vá»›i gradient checkpointing)')
    parser.add_argument('--gradient_checkpointing', action='store_true', default=False, help='Sá»­ dá»¥ng gradient checkpointing Ä‘á»ƒ tiáº¿t kiá»‡m bá»™ nhá»› (khuyáº¿n nghá»‹)')
    
    args = parser.parse_args()
    
    # Hiá»ƒn thá»‹ cáº¥u hÃ¬nh dá»±a trÃªn dataset
    dataset_info = {
        'train': ('FULL HOTPOT QA TRAIN (~90K samples)', 'training'),
        'dev': ('FULL HOTPOT QA DEV (~7.4K samples)', 'development/validation')
    }
    
    dataset_name, dataset_purpose = dataset_info[args.dataset]
    
    print(f"ğŸ“– TRAINING TRÃŠN {args.dataset.upper()} SET")
    print("=" * 60)
    print(f"ğŸ“Š Dataset: {dataset_name if args.samples is None else f'{args.samples} {args.dataset} samples'}")
    print(f"ğŸ¯ Má»¥c Ä‘Ã­ch: Sá»­ dá»¥ng dá»¯ liá»‡u {dataset_purpose} cho training")
    if args.dataset == 'dev':
        print(f"âš ï¸  LÆ¯U Ã: Training trÃªn DEV set - thÆ°á»ng dÃ¹ng cho testing/validation")
        print(f"ğŸ“ˆ CÃ³ thá»ƒ há»¯u Ã­ch cho thÃ­ nghiá»‡m nhanh hoáº·c debug")
    print(f"ğŸ”„ Epochs: {args.epochs} {'âœ… (Paper: 16)' if args.epochs == 16 else 'âš ï¸ (Paper: 16)'}")
    print(f"ğŸ“¦ Batch size: {args.batch_size} {'âœ… (Paper: 1)' if args.batch_size == 1 else 'âš ï¸ (Paper: 1)'}")
    print(f"ğŸ¯ Learning rate: {args.learning_rate} {'âœ… (Paper: 2e-5)' if args.learning_rate == 2e-5 else 'âš ï¸ (Paper: 2e-5)'}")
    print(f"ğŸ“ Max length: {args.max_len} {'âœ… (Paper: 512)' if args.max_len == 512 else 'âš ï¸ (Paper: 512)'}")
    print(f"âš¡ Mixed precision: {args.mixed_precision}")
    print(f"ğŸ”„ Gradient checkpointing: {args.gradient_checkpointing}")
    print(f"ğŸ”¢ Max batches: {'KHÃ”NG GIá»šI Háº N' if args.max_batches is None else args.max_batches}")
    
    # Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh Ä‘Æ°á»ng dáº«n lÆ°u dá»±a trÃªn dataset
    if args.save_path == 'models/deberta_v3_paper_full.pt':  # ÄÆ°á»ng dáº«n máº·c Ä‘á»‹nh
        if args.dataset == 'dev':
            args.save_path = 'models/deberta_v3_paper_dev.pt'
        else:
            args.save_path = 'models/deberta_v3_paper_train.pt'
    
    print(f"ğŸ’¾ ÄÆ°á»ng dáº«n lÆ°u: {args.save_path}")
    print("=" * 60)
    
    # Kiá»ƒm tra tuÃ¢n thá»§ paper
    is_paper_config = (
        args.samples is None and
        args.epochs == 16 and
        args.batch_size == 1 and
        args.learning_rate == 2e-5 and
        args.max_len == 512
    )
    
    if is_paper_config:
        print(f"ğŸ‰ PHÃT HIá»†N Cáº¤U HÃŒNH PAPER CHÃNH XÃC! (sá»­ dá»¥ng {args.dataset.upper()} set)")
        print(f"ğŸ“– Training vá»›i cÃ i Ä‘áº·t chÃ­nh xÃ¡c tá»« paper trÃªn dá»¯ liá»‡u {args.dataset}")
    else:
        print("âš ï¸  Cáº¥u hÃ¬nh tÃ¹y chá»‰nh - khÃ¡c vá»›i cÃ i Ä‘áº·t paper")
        print("ğŸ“ Äá»ƒ sá»­ dá»¥ng cáº¥u hÃ¬nh paper chÃ­nh xÃ¡c, cháº¡y khÃ´ng cÃ³ arguments")
    print("=" * 60)
    
    # Lá»±a chá»n thiáº¿t bá»‹
    device = get_device()
    if args.gpu and not torch.cuda.is_available():
        print("âš ï¸  YÃªu cáº§u GPU nhÆ°ng khÃ´ng cÃ³ sáºµn, sá»­ dá»¥ng CPU")
    
    # Táº£i dá»¯ liá»‡u - sá»­ dá»¥ng dataset Ä‘Ã£ chá»n
    print(f"\nğŸ“š Äang táº£i dá»¯ liá»‡u {args.dataset.upper()} cho training...")
    train_data = load_hotpot_data(args.dataset, sample_size=args.samples)
    print(f"âœ… ÄÃ£ táº£i {len(train_data)} {args.dataset.upper()} samples cho training")
    if args.dataset == 'dev':
        print(f"âš ï¸  LÆ¯U Ã: Sá»­ dá»¥ng DEV set lÃ m dá»¯ liá»‡u training!")
    
    # Táº¡o model
    print("\nğŸ§  Äang táº¡o model...")
    model = create_advanced_retriever(
        model_name="microsoft/deberta-v3-base",
        beam_size=2,
        use_focal=True,
        use_early_stop=True,
        max_seq_len=args.max_len,
        gradient_checkpointing=args.gradient_checkpointing
    )
    model.to(device)
    print(f"âœ… Model Ä‘Ã£ táº¡o vá»›i {model.count_parameters():,} parameters")
    
    # Thiáº¿t láº­p mixed precision
    scaler = None
    if args.mixed_precision and torch.cuda.is_available():
        scaler = torch.cuda.amp.GradScaler()
        print("âš¡ Sá»­ dá»¥ng mixed precision training")
    
    # Táº¡o dataset vÃ  dataloader
    print("\nğŸ“¦ Äang táº¡o dataset...")
    tokenizer = model.tokenizer

    dataset = RetrievalDataset(train_data, tokenizer, max_len=args.max_len, num_contexts=5)
    
    # Pin memory Ä‘á»ƒ chuyá»ƒn GPU nhanh hÆ¡n
    pin_memory = torch.cuda.is_available()
    num_workers = 2 if torch.cuda.is_available() else 0

    
    dataloader = DataLoader(
        dataset, 
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        pin_memory=pin_memory,
        num_workers=num_workers
    )
    
    # Optimizer vá»›i cÃ i Ä‘áº·t tá»‘t hÆ¡n cho GPU
    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=args.learning_rate, 
        weight_decay=0.01,
        eps=1e-8,
        betas=(0.9, 0.999)
    )
    
    # VÃ²ng láº·p training
    print(f"\nğŸ¯ Báº¯t Ä‘áº§u training cho {args.epochs} epochs...")
    if torch.cuda.is_available():
        print(f"ğŸ”¥ GPU Memory trÆ°á»›c khi training: {torch.cuda.memory_allocated()/1024**2:.1f}MB")
    
    best_f1 = 0.0
    best_em = 0.0
    train_metrics = []
    
    for epoch in range(args.epochs):
        print(f"\nğŸ“š Epoch {epoch+1}/{args.epochs}")
        
        # ThÃ´ng tin GPU memory
        if torch.cuda.is_available():
            print(f"ğŸ”¥ GPU Memory trÆ°á»›c epoch: {torch.cuda.memory_allocated()/1024**2:.1f}MB")
        
        max_f1, max_em, avg_loss = train_epoch(
            model=model,
            dataloader=dataloader,
            optimizer=optimizer,
            device=device,
            max_batches=args.max_batches,
            scaler=scaler
        )
        
        train_metrics.append({
            'epoch': epoch + 1,
            'max_f1': max_f1,
            'max_em': max_em,
            'avg_loss': avg_loss
        })
        
        print(f"ğŸ“Š Epoch {epoch+1} - Max F1: {max_f1:.4f}, Max EM: {max_em:.4f}, Avg Loss: {avg_loss:.4f}")
        
        # ThÃ´ng tin GPU memory
        if torch.cuda.is_available():
            print(f"ğŸ”¥ GPU Memory sau epoch: {torch.cuda.memory_allocated()/1024**2:.1f}MB")
        
        # LÆ°u model náº¿u F1 cáº£i thiá»‡n
        if max_f1 > best_f1:
            best_f1 = max_f1
            best_em = max_em
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'epoch': epoch,
                'max_f1': max_f1,
                'max_em': max_em,
                'avg_loss': avg_loss,
                'train_metrics': train_metrics,
                'config': {
                    'model_name': 'microsoft/deberta-v3-base',
                    'beam_size': 2,
                    'use_focal': True,
                    'max_seq_len': args.max_len,
                    'dataset': args.dataset,
                    'samples': args.samples,
                    'epochs': args.epochs,
                    'batch_size': args.batch_size,
                    'learning_rate': args.learning_rate
                }
            }, args.save_path)
            print(f"ğŸ’¾ Model tá»‘t nháº¥t Ä‘Ã£ lÆ°u vÃ o {args.save_path} (F1: {max_f1:.4f}, EM: {max_em:.4f})")
            print(f"ğŸ“ˆ Cáº£i thiá»‡n F1: {max_f1:.4f}")
        
        # Giáº£i thÃ­ch táº¡i sao EM cÃ³ thá»ƒ báº±ng 0
        if max_em == 0.0:
            print(f"â— EM = 0 nghÄ©a lÃ : KhÃ´ng cÃ³ exact match giá»¯a predicted vÃ  target supporting facts")
            print(f"   - F1 > 0 nghÄ©a lÃ  cÃ³ overlap má»™t pháº§n (má»™t sá»‘ dá»± Ä‘oÃ¡n Ä‘Ãºng)")
            print(f"   - Äiá»u nÃ y bÃ¬nh thÆ°á»ng trong early training - model há»c partial patterns trÆ°á»›c")
    
    print(f"\nğŸ‰ Training hoÃ n thÃ nh!")
    print(f"ğŸ“ˆ Metrics cuá»‘i - F1: {train_metrics[-1]['max_f1']:.4f}, EM: {train_metrics[-1]['max_em']:.4f}")
    print(f"ğŸ† F1 tá»‘t nháº¥t: {best_f1:.4f}, EM tá»‘t nháº¥t: {best_em:.4f}")
    print(f"ğŸ’¾ Model Ä‘Ã£ lÆ°u vÃ o {args.save_path}")
    
    # TÃ³m táº¯t dataset
    dataset_summary = f"Trained trÃªn {args.dataset.upper()} set"
    if args.dataset == 'dev':
        dataset_summary += " (dá»¯ liá»‡u development Ä‘Æ°á»£c dÃ¹ng cho training)"
    
    print(f"\nğŸ“š TÃ³m táº¯t Training:")
    print(f"â€¢ Dataset: {dataset_summary}")
    print(f"â€¢ Samples: {len(train_data):,}")
    print(f"â€¢ F1 Score: Äo overlap má»™t pháº§n giá»¯a predicted vÃ  target supporting facts")
    print(f"â€¢ EM Score: Exact Match - 1.0 chá»‰ khi predictions khá»›p chÃ­nh xÃ¡c vá»›i targets")
    print(f"â€¢ Táº¡i sao EM=0? Model Ä‘ang há»c dáº§n dáº§n - partial matches (F1>0) xuáº¥t hiá»‡n trÆ°á»›c exact matches")

if __name__ == "__main__":
    main()
