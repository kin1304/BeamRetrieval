#!/usr/bin/env python3
"""
Script Training Ch√≠nh cho Advanced Multi-Hop Retriever
C√°ch d√πng: python train.py [--dataset train/dev] [--samples N] [--epochs N] [--batch_size N] [--gpu]
"""
import argparse
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import DebertaV2Tokenizer
from tqdm import tqdm
import random

from models.advanced_retriever import create_advanced_retriever
from utils.data_loader import load_hotpot_data

def get_device():
    """L·∫•y thi·∫øt b·ªã t·ªët nh·∫•t c√≥ s·∫µn"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"üöÄ Using GPU: {torch.cuda.get_device_name()}")
        print(f"üî• GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    elif torch.backends.mps.is_available():
        device = torch.device('mps')
        print("üçé Using Apple Silicon GPU (MPS)")
    else:
        device = torch.device('cpu')
        print("üíª Using CPU")
    return device

def calculate_f1_em(predictions, targets):
    """T√≠nh to√°n F1 score v√† Exact Match"""
    if not predictions or not targets:
        return 0.0, 0.0
    
    # Chuy·ªÉn ƒë·ªïi th√†nh sets ƒë·ªÉ t√≠nh F1
    pred_set = set(predictions)
    target_set = set(targets)
    
    # Exact Match
    em = 1.0 if pred_set == target_set else 0.0
    
    # F1 Score
    if len(pred_set) == 0 and len(target_set) == 0:
        f1 = 1.0
    elif len(pred_set) == 0 or len(target_set) == 0:
        f1 = 0.0
    else:
        precision = len(pred_set & target_set) / len(pred_set)
        recall = len(pred_set & target_set) / len(target_set)
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    
    return f1, em

class RetrievalDataset(Dataset):
    """Dataset cho training multi-hop retrieval"""
    
    def __init__(self, data, tokenizer, max_len=256):
        self.data = data
        self.tokenizer = tokenizer
        self.max_len = max_len
        
    def __len__(self):
        return len(self.data)
    
    def _split_context_to_paragraphs(self, context_item):
        """
        üöÄ T·ªêI ∆ØU: HotpotQA format: [title, [sentence1, sentence2, ...]]
        G·ªôp title + t·∫•t c·∫£ sentences th√†nh 1 paragraph l·ªõn duy nh·∫•t
        """
        title = context_item[0]        # Title c·ªßa context
        sentences = context_item[1]    # List c√°c c√¢u
        
        # G·ªôp t·∫•t c·∫£ sentences th√†nh 1 ƒëo·∫°n vƒÉn l·ªõn
        combined_text = ' '.join(sentence.strip() for sentence in sentences if sentence.strip())
        
        # T·∫°o paragraph ho√†n ch·ªânh: Title + Combined text
        if combined_text:
            paragraph = f"{title}. {combined_text}"
        else:
            paragraph = f"{title}. No content available."
        
        # Return list v·ªõi 1 paragraph duy nh·∫•t
        return [paragraph]
    
    def __getitem__(self, idx):
        item = self.data[idx]
        question = item['question']
        contexts = item['contexts']
        supporting_facts = item.get('supporting_facts', [])
        
        # üÜï S·ª¨ D·ª§NG T·∫§T C·∫¢ CONTEXTS - kh√¥ng gi·ªõi h·∫°n
        selected_contexts = contexts  # S·ª≠ d·ª•ng t·∫•t c·∫£ contexts c√≥ s·∫µn
        num_contexts = len(selected_contexts)  # Dynamic cho m·ªói sample
        
        # üöÄ T·ªêI ∆ØU: Chia ƒëo·∫°n vƒÉn TR∆Ø·ªöC khi tokenization (kh√¥ng decode/re-tokenize!)
        q_tokens_list = []
        p_tokens_list = []  # M·ªõi: chu·ªói ƒëo·∫°n vƒÉn tr·ª±c ti·∫øp
        
        # Tokenize c√¢u h·ªèi m·ªôt l·∫ßn (token s·∫°ch kh√¥ng c√≥ special tokens)
        question_tokens = self.tokenizer(
            question,
            max_length=self.max_len // 2,
            truncation=True,
            add_special_tokens=False,  # Kh√¥ng c√≥ [CLS], [SEP] cho token s·∫°ch
            return_tensors='pt'
        )['input_ids'].view(-1)
        
        # ƒê·ªám question tokens t·ªõi ƒë·ªô d√†i nh·∫•t qu√°n
        q_max_len = self.max_len // 2
        if len(question_tokens) > q_max_len:
            question_tokens = question_tokens[:q_max_len]
        else:
            padding_len = q_max_len - len(question_tokens)
            question_tokens = torch.cat([question_tokens, torch.full((padding_len,), self.tokenizer.pad_token_id)])
        
        # X·ª≠ l√Ω t·ª´ng context v√† chia th√†nh ƒëo·∫°n vƒÉn
        paragraph_to_context_map = {}  # Map paragraph index -> (context_idx, paragraph text, title)
        
        for ctx_idx, ctx in enumerate(selected_contexts):  # üÜï S·ª≠ d·ª•ng t·∫•t c·∫£ selected_contexts
            # HotpotQA format: ctx = [title, [sentences]]
            title = ctx[0]
            paragraphs = self._split_context_to_paragraphs(ctx)
            
            # Tokenize t·ª´ng paragraph: [CLS] + Q + P + [SEP]
            for paragraph_text in paragraphs:
                paragraph_idx = len(p_tokens_list)  # Current index trong p_tokens_list
                paragraph_to_context_map[paragraph_idx] = (ctx_idx, paragraph_text, title)
                
                combined_text = question + " " + paragraph_text
                para_tokens = self.tokenizer(
                    combined_text,
                    max_length=self.max_len,
                    truncation=True,
                    padding='max_length',
                    return_tensors='pt'
                )['input_ids'].view(-1)
                
                p_tokens_list.append(para_tokens)
        
        # L∆∞u question tokens m·ªôt l·∫ßn (d√πng l·∫°i cho t·∫•t c·∫£ ƒëo·∫°n vƒÉn)
        q_tokens_list.append(question_tokens)
        
        # üÜï X·ª¨ L√ù SUPPORTING FACTS ‚Üí PARAGRAPH INDICES
        sf_paragraph_indices = []
        
        # Map supporting facts to paragraph indices d·ª±a tr√™n title matching
        for sf in supporting_facts:
            sf_title = sf[0]
            for paragraph_idx, (ctx_idx, paragraph_text, title) in paragraph_to_context_map.items():
                if title == sf_title:
                    sf_paragraph_indices.append(paragraph_idx)
                    break  # Ch·ªâ add paragraph ƒë·∫ßu ti√™n c·ªßa context c√≥ title kh·ªõp
        
        # ƒê·∫£m b·∫£o √≠t nh·∫•t 1 supporting fact paragraph
        if not sf_paragraph_indices:
            sf_paragraph_indices.append(0)
        
        # Remove duplicates v√† sort
        sf_paragraph_indices = sorted(list(set(sf_paragraph_indices)))
        sf_paragraph_indices = sf_paragraph_indices[:3]  # T·ªëi ƒëa 3 supporting paragraphs
        
        # üÜï KH√îNG ƒê·ªÜM - ƒë·ªÉ dynamic length
        # while len(sf_indices) < 2:
        #     sf_indices.append(sf_indices[0])
        
        return {
            'q_codes': q_tokens_list,  # Token c√¢u h·ªèi s·∫°ch ƒë∆°n (kh√¥ng c√≥ [CLS], [SEP])
            'p_codes': p_tokens_list,  # M·ªöI: Chu·ªói ƒëo·∫°n vƒÉn tr·ª±c ti·∫øp [CLS] + Q + P + [SEP]
            'sf_idx': [torch.tensor(sf_paragraph_indices, dtype=torch.long)],  # üÜï PARAGRAPH INDICES
            'hop': len(sf_paragraph_indices)
        }

def collate_fn(batch):
    """üöÄ T·ªêI ∆ØU: H√†m collate t√πy ch·ªânh cho ƒë·ªãnh d·∫°ng paragraph-based m·ªõi"""
    return {
        'q_codes': [item['q_codes'] for item in batch],
        'p_codes': [item['p_codes'] for item in batch],  # M·ªöI: Chu·ªói ƒëo·∫°n vƒÉn
        'sf_idx': [item['sf_idx'] for item in batch],
        'hops': [item['hop'] for item in batch]
    }

def train_epoch(model, dataloader, optimizer, device, max_batches=None, scaler=None):
    """Train m·ªôt epoch v·ªõi t·ªëi ∆∞u h√≥a GPU"""
    model.train()
    epoch_losses = []
    f1_scores = []
    em_scores = []
    
    # T√≠nh to√°n checkpoint 30% ƒë·ªÉ b√°o c√°o
    total_batches = max_batches if max_batches else len(dataloader)
    checkpoint_30_percent = int(total_batches * 0.3)
    
    progress_bar = tqdm(dataloader, desc="Training")
    for batch_idx, batch in enumerate(progress_bar):
        
        batch_f1 = 0
        batch_em = 0
        valid_samples = 0
        
        # X·ª≠ l√Ω t·ª´ng sample trong batch ri√™ng l·∫ª
        for i in range(len(batch['q_codes'])):
            try:
                # Zero gradients cho t·ª´ng sample
                optimizer.zero_grad()
                
                # üöÄ T·ªêI ∆ØU: Di chuy·ªÉn d·ªØ li·ªáu l√™n device hi·ªáu qu·∫£ (ƒë·ªãnh d·∫°ng m·ªõi)
                q_codes = [q.to(device, non_blocking=True) for q in batch['q_codes'][i]]
                p_codes = [p.to(device, non_blocking=True) for p in batch['p_codes'][i]]  # M·ªöI: Chu·ªói ƒëo·∫°n vƒÉn
                sf_idx = [s.to(device, non_blocking=True) for s in batch['sf_idx'][i]]
                hop = batch['hops'][i]
                
                # Mixed precision forward pass
                if scaler is not None:

                    with torch.cuda.amp.autocast('cuda'):
                        # üöÄ T·ªêI ∆ØU: S·ª≠ d·ª•ng p_codes (chu·ªói ƒëo·∫°n vƒÉn) - paragraph-only system
                        outputs = model(q_codes, p_codes, sf_idx, hop)
                        loss = outputs['loss']
                else:
                    outputs = model(q_codes, p_codes, sf_idx, hop)
                    loss = outputs['loss']
                
                if loss.requires_grad and not torch.isnan(loss):
                    # Backward pass cho sample ri√™ng l·∫ª
                    if scaler is not None:
                        scaler.scale(loss).backward()
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        loss.backward()
                        optimizer.step()
                    
                    valid_samples += 1
                    epoch_losses.append(loss.item())
                    
                    # üÜï S·ª¨ D·ª§NG PARAGRAPH_PREDS CHO EVALUATION
                    predictions = []
                    targets = sf_idx[0].cpu().tolist()  # Paragraph indices targets
                    
                    if 'paragraph_preds' in outputs and outputs['paragraph_preds']:
                        # S·ª≠ d·ª•ng paragraph predictions tr·ª±c ti·∫øp
                        predictions = outputs['paragraph_preds'][0] if len(outputs['paragraph_preds']) > 0 else []
                    
                    # Lu√¥n t√≠nh F1/EM k·ªÉ c·∫£ khi predictions r·ªóng (ƒë·ªÉ debug)
                    f1, em = calculate_f1_em(predictions, targets)
                    batch_f1 += f1
                    batch_em += em
                    f1_scores.append(f1)
                    em_scores.append(em)
                    
                    # Th√¥ng tin debug cho v√†i sample ƒë·∫ßu
                    if len(f1_scores) <= 3:
                        print(f"\nüîç Debug Sample {len(f1_scores)}:")
                        print(f"   Predictions: {predictions}")
                        print(f"   Targets: {targets}")
                        print(f"   F1: {f1:.4f}, EM: {em:.4f}")
                        print(f"   Available outputs: {list(outputs.keys())}")
                    
            except Exception as e:
                print(f"Sample {i} th·∫•t b·∫°i: {e}")
                continue
        
        if valid_samples > 0:
            avg_f1 = batch_f1 / valid_samples
            avg_em = batch_em / valid_samples
            
            progress_bar.set_postfix({
                'loss': f'{epoch_losses[-1]:.4f}' if epoch_losses else '0.0000',
                'f1': f'{avg_f1:.4f}',
                'em': f'{avg_em:.4f}',
                'valid': f'{valid_samples}/{len(batch["q_codes"])}'
            })
        
        # X√≥a GPU cache ƒë·ªãnh k·ª≥
        if torch.cuda.is_available() and batch_idx % 10 == 0:
            torch.cuda.empty_cache()
        
        # B√°o c√°o metrics theo chu k·ª≥ (m·ªói 25% v√† 50%)
        if f1_scores and em_scores:
            current_avg_f1 = sum(f1_scores) / len(f1_scores)
            current_avg_em = sum(em_scores) / len(em_scores)
            current_avg_loss = sum(epoch_losses) / len(epoch_losses)
            current_max_f1 = max(f1_scores)
            current_max_em = max(em_scores)
            
            # B√°o c√°o t·∫°i 25%, 50%, 75% 
            progress_checkpoints = [int(total_batches * p) for p in [0.25, 0.5, 0.75]]
            
            if batch_idx in progress_checkpoints:
                progress_pct = int((batch_idx / total_batches) * 100)
                print(f"{progress_pct}% ({batch_idx+1}/{total_batches}) - Loss: {current_avg_loss:.4f}, F1: {current_max_f1:.4f}, EM: {current_max_em:.4f}")
        
        # D·ª´ng s·ªõm n·∫øu max_batches ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
        if max_batches and batch_idx >= max_batches - 1:
            break
    
    max_f1 = max(f1_scores) if f1_scores else 0.0
    max_em = max(em_scores) if em_scores else 0.0
    avg_f1 = sum(f1_scores) / len(f1_scores) if f1_scores else 0.0
    avg_em = sum(em_scores) / len(em_scores) if em_scores else 0.0
    avg_loss = sum(epoch_losses) / len(epoch_losses) if epoch_losses else 0.0
    
    return max_f1, max_em, avg_f1, avg_em, avg_loss

def main():
    parser = argparse.ArgumentParser(description='Train Advanced Multi-Hop Retriever - C·∫•u h√¨nh Paper')
    # ========== C·∫§U H√åNH PAPER (GI√Å TR·ªä M·∫∂C ƒê·ªäNH) ==========
    parser.add_argument('--dataset', type=str, default='train', choices=['train', 'dev'], 
                       help='Dataset s·ª≠ d·ª•ng cho training (train ho·∫∑c dev)')
    parser.add_argument('--samples', type=int, default=None, help='S·ªë l∆∞·ª£ng training samples (None = FULL DATASET)')
    parser.add_argument('--epochs', type=int, default=2, help='S·ªë epochs (Paper: 16, Test: 2)')
    parser.add_argument('--batch_size', type=int, default=1, help='K√≠ch th∆∞·ªõc batch (Paper: 1)')
    parser.add_argument('--learning_rate', type=float, default=2e-5, help='Learning rate (Paper: 2e-5)')
    parser.add_argument('--max_len', type=int, default=512, help='ƒê·ªô d√†i chu·ªói t·ªëi ƒëa (Paper: 512)')
    parser.add_argument('--save_path', type=str, default='models/deberta_v3_paper_full.pt', help='ƒê∆∞·ªùng d·∫´n l∆∞u model')
    parser.add_argument('--gradient_accumulation', type=int, default=1, help='B∆∞·ªõc t√≠ch l≈©y gradient')
    parser.add_argument('--max_batches', type=int, default=None, help='S·ªë batch t·ªëi ƒëa m·ªói epoch (None = FULL DATASET)')
    parser.add_argument('--gpu', action='store_true', help='√âp bu·ªôc s·ª≠ d·ª•ng GPU')
    parser.add_argument('--mixed_precision', action='store_true', default=False, help='S·ª≠ d·ª•ng mixed precision (th∆∞·ªùng False v·ªõi gradient checkpointing)')
    parser.add_argument('--gradient_checkpointing', action='store_true', default=False, help='S·ª≠ d·ª•ng gradient checkpointing ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ (khuy·∫øn ngh·ªã)')
    
    args = parser.parse_args()
    
    # Hi·ªÉn th·ªã c·∫•u h√¨nh d·ª±a tr√™n dataset
    dataset_info = {
        'train': ('FULL HOTPOT QA TRAIN (~90K samples)', 'training'),
        'dev': ('FULL HOTPOT QA DEV (~7.4K samples)', 'development/validation')
    }
    
    dataset_name, dataset_purpose = dataset_info[args.dataset]
    
    # L·ª±a ch·ªçn thi·∫øt b·ªã
    device = get_device()
    # T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh ƒë∆∞·ªùng d·∫´n l∆∞u d·ª±a tr√™n dataset
    if args.save_path == 'models/deberta_v3_paper_full.pt':  # ƒê∆∞·ªùng d·∫´n m·∫∑c ƒë·ªãnh
        if args.dataset == 'dev':
            args.save_path = 'models/deberta_v3_paper_dev.pt'
        else:
            args.save_path = 'models/deberta_v3_paper_train.pt'
    
    # T·∫£i d·ªØ li·ªáu
    train_data = load_hotpot_data(args.dataset, sample_size=args.samples)
    
    # üÜï TH·ªêNG K√ä CONTEXTS - ch·ªâ ƒë·ªÉ th√¥ng tin, kh√¥ng gi·ªõi h·∫°n
    if train_data:
        context_lengths = [len(item['contexts']) for item in train_data if 'contexts' in item]
        if context_lengths:
            print(f"üìä Dataset c√≥ {len(context_lengths)} items")
            print(f"üìä Context lengths: min={min(context_lengths)}, max={max(context_lengths)}, avg={sum(context_lengths)/len(context_lengths):.1f}")
            print(f"‚ú® S·ª≠ d·ª•ng ALL contexts cho m·ªói sample (dynamic)")
        else:
            print(f"‚ö†Ô∏è  No context data found")
    else:
        print(f"‚ö†Ô∏è  No training data loaded")
    
    print(f"Training tr√™n {len(train_data)} {args.dataset.upper()} samples")
    print(f"Epochs: {args.epochs}, Batch size: {args.batch_size}, LR: {args.learning_rate}")
    print(f"Model s·∫Ω l∆∞u t·∫°i: {args.save_path}")
    print("=" * 60)
    
    # T·∫°o model
    model = create_advanced_retriever(
        model_name="microsoft/deberta-v3-base",
        beam_size=2,
        use_focal=True,
        use_early_stop=True,
        max_seq_len=args.max_len,
        gradient_checkpointing=args.gradient_checkpointing
    )
    model.to(device)
    
    # Thi·∫øt l·∫≠p mixed precision
    scaler = None
    if args.mixed_precision and torch.cuda.is_available():
        scaler = torch.cuda.amp.GradScaler()
        print("‚ö° S·ª≠ d·ª•ng mixed precision training")
    
    # T·∫°o dataset v√† dataloader
    print("\nüì¶ ƒêang t·∫°o dataset...")
    tokenizer = model.tokenizer

    dataset = RetrievalDataset(train_data, tokenizer, max_len=args.max_len)  # üÜï B·ªè num_contexts
    
    # Pin memory ƒë·ªÉ chuy·ªÉn GPU nhanh h∆°n
    pin_memory = torch.cuda.is_available()
    num_workers = 2 if torch.cuda.is_available() else 0

    
    dataloader = DataLoader(
        dataset, 
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        pin_memory=pin_memory,
        num_workers=num_workers
    )
    
    # Optimizer v·ªõi c√†i ƒë·∫∑t t·ªët h∆°n cho GPU
    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=args.learning_rate, 
        weight_decay=0.01,
        eps=1e-8,
        betas=(0.9, 0.999)
    )
    
    # V√≤ng l·∫∑p training
    print(f"\nüéØ B·∫Øt ƒë·∫ßu training cho {args.epochs} epochs...")
    if torch.cuda.is_available():
        print(f"üî• GPU Memory tr∆∞·ªõc khi training: {torch.cuda.memory_allocated()/1024**2:.1f}MB")
    
    best_f1 = 0.0
    best_em = 0.0
    train_metrics = []
    
    for epoch in range(args.epochs):
        print(f"\nüìö Epoch {epoch+1}/{args.epochs}")
        
        # Th√¥ng tin GPU memory
        if torch.cuda.is_available():
            print(f"üî• GPU Memory tr∆∞·ªõc epoch: {torch.cuda.memory_allocated()/1024**2:.1f}MB")
        
        max_f1, max_em, avg_f1, avg_em, avg_loss = train_epoch(
            model=model,
            dataloader=dataloader,
            optimizer=optimizer,
            device=device,
            max_batches=args.max_batches,
            scaler=scaler
        )
        
        train_metrics.append({
            'epoch': epoch + 1,
            'max_f1': max_f1,
            'max_em': max_em,
            'avg_f1': avg_f1,
            'avg_em': avg_em,
            'avg_loss': avg_loss
        })
        
        print(f"Epoch {epoch+1}: Max F1={max_f1:.4f}, Avg F1={avg_f1:.4f}, Max EM={max_em:.4f}, Avg EM={avg_em:.4f}, Loss={avg_loss:.4f}")
        
        # Th√¥ng tin GPU memory
        if torch.cuda.is_available():
            gpu_mem = torch.cuda.memory_allocated()/1024**2
            gpu_max = torch.cuda.max_memory_allocated()/1024**2
            print(f"üî• GPU Memory: {gpu_mem:.1f}MB (Max: {gpu_max:.1f}MB)")
            torch.cuda.reset_peak_memory_stats()
        
        # L∆∞u model n·∫øu F1 c·∫£i thi·ªán
        if max_f1 > best_f1:
            best_f1 = max_f1
            best_em = max_em
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'epoch': epoch,
                'max_f1': max_f1,
                'max_em': max_em,
                'avg_loss': avg_loss,
                'train_metrics': train_metrics,
                'config': {
                    'model_name': 'microsoft/deberta-v3-base',
                    'beam_size': 2,
                    'use_focal': True,
                    'max_seq_len': args.max_len,
                    'dataset': args.dataset,
                    'samples': args.samples,
                    'epochs': args.epochs,
                    'batch_size': args.batch_size,
                    'learning_rate': args.learning_rate
                }
            }, args.save_path)
            print(f"Model m·ªõi t·ªët nh·∫•t l∆∞u t·∫°i {args.save_path} (F1: {best_f1:.4f})")
    
    # T√≥m t·∫Øt cu·ªëi training
    final_avg_f1 = train_metrics[-1]['avg_f1'] if train_metrics else 0.0
    final_avg_em = train_metrics[-1]['avg_em'] if train_metrics else 0.0
    print(f"\nTraining ho√†n th√†nh!")
    print(f"Best Max F1: {best_f1:.4f}, Best Max EM: {best_em:.4f}")
    print(f"Final Avg F1: {final_avg_f1:.4f}, Final Avg EM: {final_avg_em:.4f}")
    print(f"Model ƒë√£ l∆∞u t·∫°i: {args.save_path}")

if __name__ == "__main__":
    main()
